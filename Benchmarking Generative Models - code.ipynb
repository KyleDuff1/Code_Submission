{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KT_4tRULwaLJ",
        "outputId": "a912b7f5-9ddf-4c42-ad2d-51ece6446a31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-gan-metrics\n",
            "  Downloading pytorch_gan_metrics-0.5.4-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting pytorch-image-generation-metrics (from pytorch-gan-metrics)\n",
            "  Downloading pytorch_image_generation_metrics-0.6.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Downloading pytorch_gan_metrics-0.5.4-py3-none-any.whl (7.9 kB)\n",
            "Downloading pytorch_image_generation_metrics-0.6.1-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pytorch-image-generation-metrics, pytorch-gan-metrics\n",
            "Successfully installed pytorch-gan-metrics-0.5.4 pytorch-image-generation-metrics-0.6.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:14<00:00, 12.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting GAN Training (Baseline Approach)...\n",
            "[Epoch 1/50][Batch 0/391]: Loss_D: 1.4084, Loss_G: 3.2505\n",
            "[Epoch 1/50][Batch 100/391]: Loss_D: 0.4596, Loss_G: 3.6500\n",
            "[Epoch 1/50][Batch 200/391]: Loss_D: 0.2984, Loss_G: 2.9557\n",
            "[Epoch 1/50][Batch 300/391]: Loss_D: 0.2308, Loss_G: 3.9310\n",
            "Saved composite generated images to ./output/gan_epoch_001.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 2/50][Batch 0/391]: Loss_D: 0.4260, Loss_G: 5.7747\n",
            "[Epoch 2/50][Batch 100/391]: Loss_D: 1.2222, Loss_G: 3.2112\n",
            "[Epoch 2/50][Batch 200/391]: Loss_D: 0.6578, Loss_G: 1.8010\n",
            "[Epoch 2/50][Batch 300/391]: Loss_D: 0.2014, Loss_G: 2.4670\n",
            "Saved composite generated images to ./output/gan_epoch_002.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 3/50][Batch 0/391]: Loss_D: 0.5236, Loss_G: 2.7999\n",
            "[Epoch 3/50][Batch 100/391]: Loss_D: 0.9169, Loss_G: 2.5793\n",
            "[Epoch 3/50][Batch 200/391]: Loss_D: 0.1177, Loss_G: 3.4534\n",
            "[Epoch 3/50][Batch 300/391]: Loss_D: 0.6643, Loss_G: 2.1039\n",
            "Saved composite generated images to ./output/gan_epoch_003.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 4/50][Batch 0/391]: Loss_D: 0.7244, Loss_G: 5.8365\n",
            "[Epoch 4/50][Batch 100/391]: Loss_D: 0.1766, Loss_G: 3.5198\n",
            "[Epoch 4/50][Batch 200/391]: Loss_D: 0.1195, Loss_G: 2.7976\n",
            "[Epoch 4/50][Batch 300/391]: Loss_D: 0.8343, Loss_G: 1.1960\n",
            "Saved composite generated images to ./output/gan_epoch_004.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 5/50][Batch 0/391]: Loss_D: 0.5349, Loss_G: 2.8899\n",
            "[Epoch 5/50][Batch 100/391]: Loss_D: 0.5075, Loss_G: 3.7931\n",
            "[Epoch 5/50][Batch 200/391]: Loss_D: 1.3819, Loss_G: 6.7118\n",
            "[Epoch 5/50][Batch 300/391]: Loss_D: 0.7884, Loss_G: 4.6121\n",
            "Saved composite generated images to ./output/gan_epoch_005.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 6/50][Batch 0/391]: Loss_D: 0.1043, Loss_G: 2.9571\n",
            "[Epoch 6/50][Batch 100/391]: Loss_D: 0.1246, Loss_G: 3.1758\n",
            "[Epoch 6/50][Batch 200/391]: Loss_D: 0.1419, Loss_G: 2.1534\n",
            "[Epoch 6/50][Batch 300/391]: Loss_D: 0.0534, Loss_G: 4.2668\n",
            "Saved composite generated images to ./output/gan_epoch_006.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 7/50][Batch 0/391]: Loss_D: 0.0186, Loss_G: 8.5883\n",
            "[Epoch 7/50][Batch 100/391]: Loss_D: 1.0779, Loss_G: 15.3832\n",
            "[Epoch 7/50][Batch 200/391]: Loss_D: 3.1850, Loss_G: 6.6149\n",
            "[Epoch 7/50][Batch 300/391]: Loss_D: 0.0096, Loss_G: 6.2189\n",
            "Saved composite generated images to ./output/gan_epoch_007.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 8/50][Batch 0/391]: Loss_D: 0.0487, Loss_G: 8.6683\n",
            "[Epoch 8/50][Batch 100/391]: Loss_D: 0.0312, Loss_G: 7.7859\n",
            "[Epoch 8/50][Batch 200/391]: Loss_D: 0.0041, Loss_G: 9.7630\n",
            "[Epoch 8/50][Batch 300/391]: Loss_D: 0.0242, Loss_G: 6.0111\n",
            "Saved composite generated images to ./output/gan_epoch_008.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 9/50][Batch 0/391]: Loss_D: 0.0127, Loss_G: 4.9799\n",
            "[Epoch 9/50][Batch 100/391]: Loss_D: 0.0364, Loss_G: 4.1144\n",
            "[Epoch 9/50][Batch 200/391]: Loss_D: 0.0066, Loss_G: 9.3295\n",
            "[Epoch 9/50][Batch 300/391]: Loss_D: 0.8656, Loss_G: 8.5243\n",
            "Saved composite generated images to ./output/gan_epoch_009.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 10/50][Batch 0/391]: Loss_D: 0.0372, Loss_G: 9.9593\n",
            "[Epoch 10/50][Batch 100/391]: Loss_D: 0.6784, Loss_G: 2.6727\n",
            "[Epoch 10/50][Batch 200/391]: Loss_D: 0.0188, Loss_G: 6.4732\n",
            "[Epoch 10/50][Batch 300/391]: Loss_D: 0.0379, Loss_G: 8.1550\n",
            "Saved composite generated images to ./output/gan_epoch_010.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 11/50][Batch 0/391]: Loss_D: 0.0432, Loss_G: 5.2916\n",
            "[Epoch 11/50][Batch 100/391]: Loss_D: 0.0062, Loss_G: 10.9564\n",
            "[Epoch 11/50][Batch 200/391]: Loss_D: 0.0018, Loss_G: 12.1392\n",
            "[Epoch 11/50][Batch 300/391]: Loss_D: 0.0164, Loss_G: 5.0487\n",
            "Saved composite generated images to ./output/gan_epoch_011.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 12/50][Batch 0/391]: Loss_D: 0.1308, Loss_G: 5.4235\n",
            "[Epoch 12/50][Batch 100/391]: Loss_D: 0.0173, Loss_G: 6.8722\n",
            "[Epoch 12/50][Batch 200/391]: Loss_D: 0.0125, Loss_G: 7.7466\n",
            "[Epoch 12/50][Batch 300/391]: Loss_D: 0.0056, Loss_G: 9.0211\n",
            "Saved composite generated images to ./output/gan_epoch_012.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 13/50][Batch 0/391]: Loss_D: 0.0121, Loss_G: 6.7705\n",
            "[Epoch 13/50][Batch 100/391]: Loss_D: 0.1422, Loss_G: 4.5088\n",
            "[Epoch 13/50][Batch 200/391]: Loss_D: 0.0173, Loss_G: 4.9993\n",
            "[Epoch 13/50][Batch 300/391]: Loss_D: 0.0027, Loss_G: 11.0162\n",
            "Saved composite generated images to ./output/gan_epoch_013.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 14/50][Batch 0/391]: Loss_D: 0.0023, Loss_G: 8.5614\n",
            "[Epoch 14/50][Batch 100/391]: Loss_D: 0.2825, Loss_G: 5.9087\n",
            "[Epoch 14/50][Batch 200/391]: Loss_D: 0.0248, Loss_G: 4.5509\n",
            "[Epoch 14/50][Batch 300/391]: Loss_D: 0.0017, Loss_G: 9.4177\n",
            "Saved composite generated images to ./output/gan_epoch_014.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 15/50][Batch 0/391]: Loss_D: 0.3663, Loss_G: 3.0357\n",
            "[Epoch 15/50][Batch 100/391]: Loss_D: 1.9376, Loss_G: 9.2564\n",
            "[Epoch 15/50][Batch 200/391]: Loss_D: 0.1165, Loss_G: 4.0204\n",
            "[Epoch 15/50][Batch 300/391]: Loss_D: 0.6073, Loss_G: 5.3334\n",
            "Saved composite generated images to ./output/gan_epoch_015.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 16/50][Batch 0/391]: Loss_D: 0.0962, Loss_G: 4.9628\n",
            "[Epoch 16/50][Batch 100/391]: Loss_D: 0.0076, Loss_G: 6.5659\n",
            "[Epoch 16/50][Batch 200/391]: Loss_D: 0.1625, Loss_G: 6.9745\n",
            "[Epoch 16/50][Batch 300/391]: Loss_D: 0.0271, Loss_G: 5.0673\n",
            "Saved composite generated images to ./output/gan_epoch_016.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 17/50][Batch 0/391]: Loss_D: 0.1036, Loss_G: 4.0965\n",
            "[Epoch 17/50][Batch 100/391]: Loss_D: 0.0480, Loss_G: 8.9148\n",
            "[Epoch 17/50][Batch 200/391]: Loss_D: 0.0483, Loss_G: 6.3744\n",
            "[Epoch 17/50][Batch 300/391]: Loss_D: 0.0013, Loss_G: 9.6179\n",
            "Saved composite generated images to ./output/gan_epoch_017.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 18/50][Batch 0/391]: Loss_D: 0.0207, Loss_G: 12.2894\n",
            "[Epoch 18/50][Batch 100/391]: Loss_D: 0.1787, Loss_G: 4.1331\n",
            "[Epoch 18/50][Batch 200/391]: Loss_D: 0.4306, Loss_G: 5.7583\n",
            "[Epoch 18/50][Batch 300/391]: Loss_D: 0.5362, Loss_G: 10.6842\n",
            "Saved composite generated images to ./output/gan_epoch_018.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 19/50][Batch 0/391]: Loss_D: 0.2277, Loss_G: 4.0931\n",
            "[Epoch 19/50][Batch 100/391]: Loss_D: 0.0274, Loss_G: 7.6581\n",
            "[Epoch 19/50][Batch 200/391]: Loss_D: 0.0216, Loss_G: 6.5568\n",
            "[Epoch 19/50][Batch 300/391]: Loss_D: 0.0793, Loss_G: 4.6175\n",
            "Saved composite generated images to ./output/gan_epoch_019.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 20/50][Batch 0/391]: Loss_D: 0.0641, Loss_G: 5.0373\n",
            "[Epoch 20/50][Batch 100/391]: Loss_D: 0.2586, Loss_G: 4.3732\n",
            "[Epoch 20/50][Batch 200/391]: Loss_D: 0.1053, Loss_G: 4.5574\n",
            "[Epoch 20/50][Batch 300/391]: Loss_D: 0.0489, Loss_G: 4.7646\n",
            "Saved composite generated images to ./output/gan_epoch_020.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 21/50][Batch 0/391]: Loss_D: 0.0083, Loss_G: 5.8513\n",
            "[Epoch 21/50][Batch 100/391]: Loss_D: 0.3121, Loss_G: 4.1249\n",
            "[Epoch 21/50][Batch 200/391]: Loss_D: 0.0139, Loss_G: 6.0307\n",
            "[Epoch 21/50][Batch 300/391]: Loss_D: 0.0063, Loss_G: 6.6900\n",
            "Saved composite generated images to ./output/gan_epoch_021.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 22/50][Batch 0/391]: Loss_D: 0.2109, Loss_G: 4.7994\n",
            "[Epoch 22/50][Batch 100/391]: Loss_D: 0.1792, Loss_G: 3.8207\n",
            "[Epoch 22/50][Batch 200/391]: Loss_D: 0.2476, Loss_G: 4.9549\n",
            "[Epoch 22/50][Batch 300/391]: Loss_D: 0.0943, Loss_G: 4.3133\n",
            "Saved composite generated images to ./output/gan_epoch_022.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 23/50][Batch 0/391]: Loss_D: 0.9560, Loss_G: 3.5065\n",
            "[Epoch 23/50][Batch 100/391]: Loss_D: 0.3645, Loss_G: 5.1257\n",
            "[Epoch 23/50][Batch 200/391]: Loss_D: 0.0500, Loss_G: 4.7177\n",
            "[Epoch 23/50][Batch 300/391]: Loss_D: 0.2776, Loss_G: 3.8363\n",
            "Saved composite generated images to ./output/gan_epoch_023.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 24/50][Batch 0/391]: Loss_D: 0.4031, Loss_G: 3.1249\n",
            "[Epoch 24/50][Batch 100/391]: Loss_D: 2.2444, Loss_G: 4.9740\n",
            "[Epoch 24/50][Batch 200/391]: Loss_D: 0.4983, Loss_G: 8.2030\n",
            "[Epoch 24/50][Batch 300/391]: Loss_D: 0.4204, Loss_G: 7.2998\n",
            "Saved composite generated images to ./output/gan_epoch_024.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 25/50][Batch 0/391]: Loss_D: 0.0303, Loss_G: 4.8429\n",
            "[Epoch 25/50][Batch 100/391]: Loss_D: 0.1320, Loss_G: 3.8621\n",
            "[Epoch 25/50][Batch 200/391]: Loss_D: 0.1726, Loss_G: 2.6072\n",
            "[Epoch 25/50][Batch 300/391]: Loss_D: 0.1322, Loss_G: 4.6413\n",
            "Saved composite generated images to ./output/gan_epoch_025.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 26/50][Batch 0/391]: Loss_D: 0.4383, Loss_G: 2.9692\n",
            "[Epoch 26/50][Batch 100/391]: Loss_D: 0.1209, Loss_G: 3.5436\n",
            "[Epoch 26/50][Batch 200/391]: Loss_D: 0.3251, Loss_G: 9.3023\n",
            "[Epoch 26/50][Batch 300/391]: Loss_D: 0.1848, Loss_G: 3.8049\n",
            "Saved composite generated images to ./output/gan_epoch_026.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 27/50][Batch 0/391]: Loss_D: 0.0185, Loss_G: 8.7989\n",
            "[Epoch 27/50][Batch 100/391]: Loss_D: 0.3106, Loss_G: 3.2077\n",
            "[Epoch 27/50][Batch 200/391]: Loss_D: 0.1540, Loss_G: 3.2333\n",
            "[Epoch 27/50][Batch 300/391]: Loss_D: 0.0025, Loss_G: 9.7688\n",
            "Saved composite generated images to ./output/gan_epoch_027.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 28/50][Batch 0/391]: Loss_D: 0.0077, Loss_G: 9.7481\n",
            "[Epoch 28/50][Batch 100/391]: Loss_D: 0.1316, Loss_G: 6.7546\n",
            "[Epoch 28/50][Batch 200/391]: Loss_D: 0.1733, Loss_G: 5.9417\n",
            "[Epoch 28/50][Batch 300/391]: Loss_D: 0.2102, Loss_G: 2.4101\n",
            "Saved composite generated images to ./output/gan_epoch_028.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 29/50][Batch 0/391]: Loss_D: 0.5487, Loss_G: 2.0711\n",
            "[Epoch 29/50][Batch 100/391]: Loss_D: 0.0492, Loss_G: 4.1453\n",
            "[Epoch 29/50][Batch 200/391]: Loss_D: 0.0412, Loss_G: 5.2467\n",
            "[Epoch 29/50][Batch 300/391]: Loss_D: 0.0542, Loss_G: 4.6441\n",
            "Saved composite generated images to ./output/gan_epoch_029.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 30/50][Batch 0/391]: Loss_D: 0.0391, Loss_G: 4.9093\n",
            "[Epoch 30/50][Batch 100/391]: Loss_D: 0.3436, Loss_G: 3.2062\n",
            "[Epoch 30/50][Batch 200/391]: Loss_D: 0.1607, Loss_G: 4.5273\n",
            "[Epoch 30/50][Batch 300/391]: Loss_D: 1.1476, Loss_G: 4.2561\n",
            "Saved composite generated images to ./output/gan_epoch_030.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 31/50][Batch 0/391]: Loss_D: 0.0023, Loss_G: 7.5473\n",
            "[Epoch 31/50][Batch 100/391]: Loss_D: 0.3062, Loss_G: 3.7711\n",
            "[Epoch 31/50][Batch 200/391]: Loss_D: 0.8947, Loss_G: 6.9056\n",
            "[Epoch 31/50][Batch 300/391]: Loss_D: 1.7516, Loss_G: 4.1014\n",
            "Saved composite generated images to ./output/gan_epoch_031.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 32/50][Batch 0/391]: Loss_D: 0.3429, Loss_G: 3.0806\n",
            "[Epoch 32/50][Batch 100/391]: Loss_D: 0.1048, Loss_G: 4.2603\n",
            "[Epoch 32/50][Batch 200/391]: Loss_D: 0.1099, Loss_G: 4.4097\n",
            "[Epoch 32/50][Batch 300/391]: Loss_D: 0.3560, Loss_G: 1.6183\n",
            "Saved composite generated images to ./output/gan_epoch_032.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 33/50][Batch 0/391]: Loss_D: 0.4500, Loss_G: 2.5451\n",
            "[Epoch 33/50][Batch 100/391]: Loss_D: 0.0162, Loss_G: 5.7103\n",
            "[Epoch 33/50][Batch 200/391]: Loss_D: 0.0040, Loss_G: 7.2800\n",
            "[Epoch 33/50][Batch 300/391]: Loss_D: 0.1042, Loss_G: 5.1551\n",
            "Saved composite generated images to ./output/gan_epoch_033.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 34/50][Batch 0/391]: Loss_D: 0.0698, Loss_G: 5.3784\n",
            "[Epoch 34/50][Batch 100/391]: Loss_D: 0.1153, Loss_G: 5.1347\n",
            "[Epoch 34/50][Batch 200/391]: Loss_D: 0.1538, Loss_G: 3.9249\n",
            "[Epoch 34/50][Batch 300/391]: Loss_D: 0.3056, Loss_G: 3.4484\n",
            "Saved composite generated images to ./output/gan_epoch_034.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 35/50][Batch 0/391]: Loss_D: 0.0340, Loss_G: 7.3541\n",
            "[Epoch 35/50][Batch 100/391]: Loss_D: 0.4649, Loss_G: 5.2331\n",
            "[Epoch 35/50][Batch 200/391]: Loss_D: 0.0324, Loss_G: 6.4903\n",
            "[Epoch 35/50][Batch 300/391]: Loss_D: 0.0187, Loss_G: 7.4911\n",
            "Saved composite generated images to ./output/gan_epoch_035.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 36/50][Batch 0/391]: Loss_D: 0.0209, Loss_G: 5.0967\n",
            "[Epoch 36/50][Batch 100/391]: Loss_D: 0.1042, Loss_G: 4.4077\n",
            "[Epoch 36/50][Batch 200/391]: Loss_D: 0.0229, Loss_G: 5.4812\n",
            "[Epoch 36/50][Batch 300/391]: Loss_D: 0.0669, Loss_G: 4.7821\n",
            "Saved composite generated images to ./output/gan_epoch_036.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 37/50][Batch 0/391]: Loss_D: 0.0287, Loss_G: 7.0628\n",
            "[Epoch 37/50][Batch 100/391]: Loss_D: 0.0241, Loss_G: 8.7857\n",
            "[Epoch 37/50][Batch 200/391]: Loss_D: 0.4059, Loss_G: 2.7818\n",
            "[Epoch 37/50][Batch 300/391]: Loss_D: 0.2356, Loss_G: 3.7060\n",
            "Saved composite generated images to ./output/gan_epoch_037.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 38/50][Batch 0/391]: Loss_D: 0.0209, Loss_G: 7.4271\n",
            "[Epoch 38/50][Batch 100/391]: Loss_D: 0.3057, Loss_G: 3.6920\n",
            "[Epoch 38/50][Batch 200/391]: Loss_D: 0.0127, Loss_G: 5.6114\n",
            "[Epoch 38/50][Batch 300/391]: Loss_D: 0.0772, Loss_G: 5.5296\n",
            "Saved composite generated images to ./output/gan_epoch_038.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 39/50][Batch 0/391]: Loss_D: 0.0022, Loss_G: 8.9545\n",
            "[Epoch 39/50][Batch 100/391]: Loss_D: 0.4017, Loss_G: 2.4453\n",
            "[Epoch 39/50][Batch 200/391]: Loss_D: 2.6644, Loss_G: 0.3081\n",
            "[Epoch 39/50][Batch 300/391]: Loss_D: 0.4608, Loss_G: 1.6446\n",
            "Saved composite generated images to ./output/gan_epoch_039.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 40/50][Batch 0/391]: Loss_D: 0.0122, Loss_G: 5.2440\n",
            "[Epoch 40/50][Batch 100/391]: Loss_D: 0.3247, Loss_G: 3.8034\n",
            "[Epoch 40/50][Batch 200/391]: Loss_D: 0.1968, Loss_G: 3.9494\n",
            "[Epoch 40/50][Batch 300/391]: Loss_D: 0.3339, Loss_G: 2.7790\n",
            "Saved composite generated images to ./output/gan_epoch_040.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 41/50][Batch 0/391]: Loss_D: 0.0119, Loss_G: 8.7132\n",
            "[Epoch 41/50][Batch 100/391]: Loss_D: 0.0525, Loss_G: 4.9407\n",
            "[Epoch 41/50][Batch 200/391]: Loss_D: 0.0023, Loss_G: 8.8203\n",
            "[Epoch 41/50][Batch 300/391]: Loss_D: 0.0029, Loss_G: 9.8563\n",
            "Saved composite generated images to ./output/gan_epoch_041.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 42/50][Batch 0/391]: Loss_D: 0.0014, Loss_G: 7.4582\n",
            "[Epoch 42/50][Batch 100/391]: Loss_D: 2.6724, Loss_G: 16.8909\n",
            "[Epoch 42/50][Batch 200/391]: Loss_D: 0.1059, Loss_G: 4.6217\n",
            "[Epoch 42/50][Batch 300/391]: Loss_D: 0.0604, Loss_G: 5.2878\n",
            "Saved composite generated images to ./output/gan_epoch_042.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 43/50][Batch 0/391]: Loss_D: 0.0545, Loss_G: 5.1303\n",
            "[Epoch 43/50][Batch 100/391]: Loss_D: 0.2377, Loss_G: 6.4630\n",
            "[Epoch 43/50][Batch 200/391]: Loss_D: 0.2343, Loss_G: 3.6344\n",
            "[Epoch 43/50][Batch 300/391]: Loss_D: 0.2783, Loss_G: 3.5123\n",
            "Saved composite generated images to ./output/gan_epoch_043.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 44/50][Batch 0/391]: Loss_D: 0.1003, Loss_G: 5.2591\n",
            "[Epoch 44/50][Batch 100/391]: Loss_D: 0.0441, Loss_G: 6.1799\n",
            "[Epoch 44/50][Batch 200/391]: Loss_D: 0.0107, Loss_G: 5.4110\n",
            "[Epoch 44/50][Batch 300/391]: Loss_D: 0.0205, Loss_G: 5.5861\n",
            "Saved composite generated images to ./output/gan_epoch_044.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 45/50][Batch 0/391]: Loss_D: 1.0762, Loss_G: 0.6391\n",
            "[Epoch 45/50][Batch 100/391]: Loss_D: 0.4018, Loss_G: 2.9380\n",
            "[Epoch 45/50][Batch 200/391]: Loss_D: 0.2483, Loss_G: 3.8827\n",
            "[Epoch 45/50][Batch 300/391]: Loss_D: 0.0970, Loss_G: 3.4689\n",
            "Saved composite generated images to ./output/gan_epoch_045.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 46/50][Batch 0/391]: Loss_D: 0.3900, Loss_G: 5.9472\n",
            "[Epoch 46/50][Batch 100/391]: Loss_D: 0.0679, Loss_G: 4.5024\n",
            "[Epoch 46/50][Batch 200/391]: Loss_D: 0.1547, Loss_G: 3.4100\n",
            "[Epoch 46/50][Batch 300/391]: Loss_D: 0.0541, Loss_G: 4.7743\n",
            "Saved composite generated images to ./output/gan_epoch_046.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 47/50][Batch 0/391]: Loss_D: 0.0398, Loss_G: 5.0054\n",
            "[Epoch 47/50][Batch 100/391]: Loss_D: 0.0104, Loss_G: 5.8894\n",
            "[Epoch 47/50][Batch 200/391]: Loss_D: 0.2627, Loss_G: 2.9703\n",
            "[Epoch 47/50][Batch 300/391]: Loss_D: 0.3915, Loss_G: 2.5956\n",
            "Saved composite generated images to ./output/gan_epoch_047.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 48/50][Batch 0/391]: Loss_D: 0.0475, Loss_G: 5.0894\n",
            "[Epoch 48/50][Batch 100/391]: Loss_D: 0.0919, Loss_G: 3.8304\n",
            "[Epoch 48/50][Batch 200/391]: Loss_D: 0.1359, Loss_G: 3.2534\n",
            "[Epoch 48/50][Batch 300/391]: Loss_D: 0.0560, Loss_G: 5.5009\n",
            "Saved composite generated images to ./output/gan_epoch_048.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 49/50][Batch 0/391]: Loss_D: 0.0045, Loss_G: 7.9850\n",
            "[Epoch 49/50][Batch 100/391]: Loss_D: 0.7460, Loss_G: 3.5764\n",
            "[Epoch 49/50][Batch 200/391]: Loss_D: 0.6290, Loss_G: 3.0317\n",
            "[Epoch 49/50][Batch 300/391]: Loss_D: 0.3054, Loss_G: 3.6025\n",
            "Saved composite generated images to ./output/gan_epoch_049.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "[Epoch 50/50][Batch 0/391]: Loss_D: 0.2668, Loss_G: 3.2588\n",
            "[Epoch 50/50][Batch 100/391]: Loss_D: 0.0057, Loss_G: 7.7287\n",
            "[Epoch 50/50][Batch 200/391]: Loss_D: 0.2590, Loss_G: 3.3238\n",
            "[Epoch 50/50][Batch 300/391]: Loss_D: 0.0222, Loss_G: 9.1690\n",
            "Saved composite generated images to ./output/gan_epoch_050.png\n",
            "Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\n",
            "GAN Training Complete.\n",
            "Starting VAE Training (Baseline Approach)...\n",
            "[VAE][Epoch 1/50][Batch 0/391]: Loss per image: 7307.0317\n",
            "[VAE][Epoch 1/50][Batch 100/391]: Loss per image: 1081.5487\n",
            "[VAE][Epoch 1/50][Batch 200/391]: Loss per image: 767.8389\n",
            "[VAE][Epoch 1/50][Batch 300/391]: Loss per image: 707.1591\n",
            "[VAE] Epoch [1/50] Average Loss per Image: 1117.5026\n",
            "[VAE][Epoch 2/50][Batch 0/391]: Loss per image: 619.2262\n",
            "[VAE][Epoch 2/50][Batch 100/391]: Loss per image: 604.5273\n",
            "[VAE][Epoch 2/50][Batch 200/391]: Loss per image: 544.3768\n",
            "[VAE][Epoch 2/50][Batch 300/391]: Loss per image: 553.4935\n",
            "[VAE] Epoch [2/50] Average Loss per Image: 556.5933\n",
            "[VAE][Epoch 3/50][Batch 0/391]: Loss per image: 537.4174\n",
            "[VAE][Epoch 3/50][Batch 100/391]: Loss per image: 480.6483\n",
            "[VAE][Epoch 3/50][Batch 200/391]: Loss per image: 477.3647\n",
            "[VAE][Epoch 3/50][Batch 300/391]: Loss per image: 456.9317\n",
            "[VAE] Epoch [3/50] Average Loss per Image: 483.3382\n",
            "[VAE][Epoch 4/50][Batch 0/391]: Loss per image: 476.2568\n",
            "[VAE][Epoch 4/50][Batch 100/391]: Loss per image: 450.9985\n",
            "[VAE][Epoch 4/50][Batch 200/391]: Loss per image: 449.3473\n",
            "[VAE][Epoch 4/50][Batch 300/391]: Loss per image: 456.6506\n",
            "[VAE] Epoch [4/50] Average Loss per Image: 449.3247\n",
            "[VAE][Epoch 5/50][Batch 0/391]: Loss per image: 449.7316\n",
            "[VAE][Epoch 5/50][Batch 100/391]: Loss per image: 417.9855\n",
            "[VAE][Epoch 5/50][Batch 200/391]: Loss per image: 444.8053\n",
            "[VAE][Epoch 5/50][Batch 300/391]: Loss per image: 418.8918\n",
            "[VAE] Epoch [5/50] Average Loss per Image: 431.9649\n",
            "[VAE][Epoch 6/50][Batch 0/391]: Loss per image: 448.3760\n",
            "[VAE][Epoch 6/50][Batch 100/391]: Loss per image: 401.6076\n",
            "[VAE][Epoch 6/50][Batch 200/391]: Loss per image: 408.1979\n",
            "[VAE][Epoch 6/50][Batch 300/391]: Loss per image: 424.7199\n",
            "[VAE] Epoch [6/50] Average Loss per Image: 420.1831\n",
            "[VAE][Epoch 7/50][Batch 0/391]: Loss per image: 435.1641\n",
            "[VAE][Epoch 7/50][Batch 100/391]: Loss per image: 408.0017\n",
            "[VAE][Epoch 7/50][Batch 200/391]: Loss per image: 404.4344\n",
            "[VAE][Epoch 7/50][Batch 300/391]: Loss per image: 401.3912\n",
            "[VAE] Epoch [7/50] Average Loss per Image: 412.2395\n",
            "[VAE][Epoch 8/50][Batch 0/391]: Loss per image: 410.9690\n",
            "[VAE][Epoch 8/50][Batch 100/391]: Loss per image: 427.9225\n",
            "[VAE][Epoch 8/50][Batch 200/391]: Loss per image: 383.8455\n",
            "[VAE][Epoch 8/50][Batch 300/391]: Loss per image: 396.3800\n",
            "[VAE] Epoch [8/50] Average Loss per Image: 406.1072\n",
            "[VAE][Epoch 9/50][Batch 0/391]: Loss per image: 392.7844\n",
            "[VAE][Epoch 9/50][Batch 100/391]: Loss per image: 412.7185\n",
            "[VAE][Epoch 9/50][Batch 200/391]: Loss per image: 399.3669\n",
            "[VAE][Epoch 9/50][Batch 300/391]: Loss per image: 405.0683\n",
            "[VAE] Epoch [9/50] Average Loss per Image: 402.0815\n",
            "[VAE][Epoch 10/50][Batch 0/391]: Loss per image: 417.6975\n",
            "[VAE][Epoch 10/50][Batch 100/391]: Loss per image: 401.4399\n",
            "[VAE][Epoch 10/50][Batch 200/391]: Loss per image: 382.6294\n",
            "[VAE][Epoch 10/50][Batch 300/391]: Loss per image: 390.8381\n",
            "[VAE] Epoch [10/50] Average Loss per Image: 396.6758\n",
            "[VAE][Epoch 11/50][Batch 0/391]: Loss per image: 392.3417\n",
            "[VAE][Epoch 11/50][Batch 100/391]: Loss per image: 395.1207\n",
            "[VAE][Epoch 11/50][Batch 200/391]: Loss per image: 389.4962\n",
            "[VAE][Epoch 11/50][Batch 300/391]: Loss per image: 390.0972\n",
            "[VAE] Epoch [11/50] Average Loss per Image: 391.4320\n",
            "[VAE][Epoch 12/50][Batch 0/391]: Loss per image: 391.3302\n",
            "[VAE][Epoch 12/50][Batch 100/391]: Loss per image: 387.6229\n",
            "[VAE][Epoch 12/50][Batch 200/391]: Loss per image: 394.5426\n",
            "[VAE][Epoch 12/50][Batch 300/391]: Loss per image: 382.7892\n",
            "[VAE] Epoch [12/50] Average Loss per Image: 388.5002\n",
            "[VAE][Epoch 13/50][Batch 0/391]: Loss per image: 386.9579\n",
            "[VAE][Epoch 13/50][Batch 100/391]: Loss per image: 394.8256\n",
            "[VAE][Epoch 13/50][Batch 200/391]: Loss per image: 399.2964\n",
            "[VAE][Epoch 13/50][Batch 300/391]: Loss per image: 374.3268\n",
            "[VAE] Epoch [13/50] Average Loss per Image: 384.7339\n",
            "[VAE][Epoch 14/50][Batch 0/391]: Loss per image: 372.7318\n",
            "[VAE][Epoch 14/50][Batch 100/391]: Loss per image: 375.3699\n",
            "[VAE][Epoch 14/50][Batch 200/391]: Loss per image: 394.5205\n",
            "[VAE][Epoch 14/50][Batch 300/391]: Loss per image: 373.2030\n",
            "[VAE] Epoch [14/50] Average Loss per Image: 380.8444\n",
            "[VAE][Epoch 15/50][Batch 0/391]: Loss per image: 364.9062\n",
            "[VAE][Epoch 15/50][Batch 100/391]: Loss per image: 366.5142\n",
            "[VAE][Epoch 15/50][Batch 200/391]: Loss per image: 381.0079\n",
            "[VAE][Epoch 15/50][Batch 300/391]: Loss per image: 378.5228\n",
            "[VAE] Epoch [15/50] Average Loss per Image: 379.0748\n",
            "[VAE][Epoch 16/50][Batch 0/391]: Loss per image: 377.5530\n",
            "[VAE][Epoch 16/50][Batch 100/391]: Loss per image: 368.1553\n",
            "[VAE][Epoch 16/50][Batch 200/391]: Loss per image: 368.8179\n",
            "[VAE][Epoch 16/50][Batch 300/391]: Loss per image: 385.2913\n",
            "[VAE] Epoch [16/50] Average Loss per Image: 376.8525\n",
            "[VAE][Epoch 17/50][Batch 0/391]: Loss per image: 383.7213\n",
            "[VAE][Epoch 17/50][Batch 100/391]: Loss per image: 358.1234\n",
            "[VAE][Epoch 17/50][Batch 200/391]: Loss per image: 373.6028\n",
            "[VAE][Epoch 17/50][Batch 300/391]: Loss per image: 384.3524\n",
            "[VAE] Epoch [17/50] Average Loss per Image: 374.4641\n",
            "[VAE][Epoch 18/50][Batch 0/391]: Loss per image: 363.6881\n",
            "[VAE][Epoch 18/50][Batch 100/391]: Loss per image: 368.4326\n",
            "[VAE][Epoch 18/50][Batch 200/391]: Loss per image: 393.2191\n",
            "[VAE][Epoch 18/50][Batch 300/391]: Loss per image: 368.7368\n",
            "[VAE] Epoch [18/50] Average Loss per Image: 371.4597\n",
            "[VAE][Epoch 19/50][Batch 0/391]: Loss per image: 378.6572\n",
            "[VAE][Epoch 19/50][Batch 100/391]: Loss per image: 371.0800\n",
            "[VAE][Epoch 19/50][Batch 200/391]: Loss per image: 369.5273\n",
            "[VAE][Epoch 19/50][Batch 300/391]: Loss per image: 360.9724\n",
            "[VAE] Epoch [19/50] Average Loss per Image: 369.5468\n",
            "[VAE][Epoch 20/50][Batch 0/391]: Loss per image: 364.7044\n",
            "[VAE][Epoch 20/50][Batch 100/391]: Loss per image: 371.4673\n",
            "[VAE][Epoch 20/50][Batch 200/391]: Loss per image: 373.4762\n",
            "[VAE][Epoch 20/50][Batch 300/391]: Loss per image: 353.7228\n",
            "[VAE] Epoch [20/50] Average Loss per Image: 368.8588\n",
            "[VAE][Epoch 21/50][Batch 0/391]: Loss per image: 374.7832\n",
            "[VAE][Epoch 21/50][Batch 100/391]: Loss per image: 372.0140\n",
            "[VAE][Epoch 21/50][Batch 200/391]: Loss per image: 390.5463\n",
            "[VAE][Epoch 21/50][Batch 300/391]: Loss per image: 365.5385\n",
            "[VAE] Epoch [21/50] Average Loss per Image: 366.7017\n",
            "[VAE][Epoch 22/50][Batch 0/391]: Loss per image: 367.6003\n",
            "[VAE][Epoch 22/50][Batch 100/391]: Loss per image: 357.7595\n",
            "[VAE][Epoch 22/50][Batch 200/391]: Loss per image: 342.9653\n",
            "[VAE][Epoch 22/50][Batch 300/391]: Loss per image: 358.7700\n",
            "[VAE] Epoch [22/50] Average Loss per Image: 365.6052\n",
            "[VAE][Epoch 23/50][Batch 0/391]: Loss per image: 372.8434\n",
            "[VAE][Epoch 23/50][Batch 100/391]: Loss per image: 369.7783\n",
            "[VAE][Epoch 23/50][Batch 200/391]: Loss per image: 344.8661\n",
            "[VAE][Epoch 23/50][Batch 300/391]: Loss per image: 362.9930\n",
            "[VAE] Epoch [23/50] Average Loss per Image: 363.1186\n",
            "[VAE][Epoch 24/50][Batch 0/391]: Loss per image: 371.8354\n",
            "[VAE][Epoch 24/50][Batch 100/391]: Loss per image: 368.6044\n",
            "[VAE][Epoch 24/50][Batch 200/391]: Loss per image: 355.2966\n",
            "[VAE][Epoch 24/50][Batch 300/391]: Loss per image: 360.2158\n",
            "[VAE] Epoch [24/50] Average Loss per Image: 362.8987\n",
            "[VAE][Epoch 25/50][Batch 0/391]: Loss per image: 356.4413\n",
            "[VAE][Epoch 25/50][Batch 100/391]: Loss per image: 372.5828\n",
            "[VAE][Epoch 25/50][Batch 200/391]: Loss per image: 368.2420\n",
            "[VAE][Epoch 25/50][Batch 300/391]: Loss per image: 365.2919\n",
            "[VAE] Epoch [25/50] Average Loss per Image: 361.1218\n",
            "[VAE][Epoch 26/50][Batch 0/391]: Loss per image: 367.5752\n",
            "[VAE][Epoch 26/50][Batch 100/391]: Loss per image: 369.0224\n",
            "[VAE][Epoch 26/50][Batch 200/391]: Loss per image: 360.0650\n",
            "[VAE][Epoch 26/50][Batch 300/391]: Loss per image: 354.9450\n",
            "[VAE] Epoch [26/50] Average Loss per Image: 360.3521\n",
            "[VAE][Epoch 27/50][Batch 0/391]: Loss per image: 343.5363\n",
            "[VAE][Epoch 27/50][Batch 100/391]: Loss per image: 333.8019\n",
            "[VAE][Epoch 27/50][Batch 200/391]: Loss per image: 344.6549\n",
            "[VAE][Epoch 27/50][Batch 300/391]: Loss per image: 339.7784\n",
            "[VAE] Epoch [27/50] Average Loss per Image: 358.2203\n",
            "[VAE][Epoch 28/50][Batch 0/391]: Loss per image: 364.3085\n",
            "[VAE][Epoch 28/50][Batch 100/391]: Loss per image: 364.9218\n",
            "[VAE][Epoch 28/50][Batch 200/391]: Loss per image: 372.7462\n",
            "[VAE][Epoch 28/50][Batch 300/391]: Loss per image: 362.4507\n",
            "[VAE] Epoch [28/50] Average Loss per Image: 358.3303\n",
            "[VAE][Epoch 29/50][Batch 0/391]: Loss per image: 341.0783\n",
            "[VAE][Epoch 29/50][Batch 100/391]: Loss per image: 357.6567\n",
            "[VAE][Epoch 29/50][Batch 200/391]: Loss per image: 349.5469\n",
            "[VAE][Epoch 29/50][Batch 300/391]: Loss per image: 353.6996\n",
            "[VAE] Epoch [29/50] Average Loss per Image: 357.0691\n",
            "[VAE][Epoch 30/50][Batch 0/391]: Loss per image: 350.2343\n",
            "[VAE][Epoch 30/50][Batch 100/391]: Loss per image: 352.5052\n",
            "[VAE][Epoch 30/50][Batch 200/391]: Loss per image: 357.1253\n",
            "[VAE][Epoch 30/50][Batch 300/391]: Loss per image: 367.4735\n",
            "[VAE] Epoch [30/50] Average Loss per Image: 356.4106\n",
            "[VAE][Epoch 31/50][Batch 0/391]: Loss per image: 349.5777\n",
            "[VAE][Epoch 31/50][Batch 100/391]: Loss per image: 347.5701\n",
            "[VAE][Epoch 31/50][Batch 200/391]: Loss per image: 367.8290\n",
            "[VAE][Epoch 31/50][Batch 300/391]: Loss per image: 350.6631\n",
            "[VAE] Epoch [31/50] Average Loss per Image: 355.8245\n",
            "[VAE][Epoch 32/50][Batch 0/391]: Loss per image: 347.4362\n",
            "[VAE][Epoch 32/50][Batch 100/391]: Loss per image: 360.3550\n",
            "[VAE][Epoch 32/50][Batch 200/391]: Loss per image: 339.1688\n",
            "[VAE][Epoch 32/50][Batch 300/391]: Loss per image: 352.5455\n",
            "[VAE] Epoch [32/50] Average Loss per Image: 354.6765\n",
            "[VAE][Epoch 33/50][Batch 0/391]: Loss per image: 358.9679\n",
            "[VAE][Epoch 33/50][Batch 100/391]: Loss per image: 361.5093\n",
            "[VAE][Epoch 33/50][Batch 200/391]: Loss per image: 350.8092\n",
            "[VAE][Epoch 33/50][Batch 300/391]: Loss per image: 352.4352\n",
            "[VAE] Epoch [33/50] Average Loss per Image: 354.1602\n",
            "[VAE][Epoch 34/50][Batch 0/391]: Loss per image: 335.6924\n",
            "[VAE][Epoch 34/50][Batch 100/391]: Loss per image: 337.4161\n",
            "[VAE][Epoch 34/50][Batch 200/391]: Loss per image: 350.6569\n",
            "[VAE][Epoch 34/50][Batch 300/391]: Loss per image: 332.8199\n",
            "[VAE] Epoch [34/50] Average Loss per Image: 351.9577\n",
            "[VAE][Epoch 35/50][Batch 0/391]: Loss per image: 360.3954\n",
            "[VAE][Epoch 35/50][Batch 100/391]: Loss per image: 345.3229\n",
            "[VAE][Epoch 35/50][Batch 200/391]: Loss per image: 344.4801\n",
            "[VAE][Epoch 35/50][Batch 300/391]: Loss per image: 348.8717\n",
            "[VAE] Epoch [35/50] Average Loss per Image: 351.4802\n",
            "[VAE][Epoch 36/50][Batch 0/391]: Loss per image: 351.3814\n",
            "[VAE][Epoch 36/50][Batch 100/391]: Loss per image: 362.1044\n",
            "[VAE][Epoch 36/50][Batch 200/391]: Loss per image: 352.6907\n",
            "[VAE][Epoch 36/50][Batch 300/391]: Loss per image: 355.4503\n",
            "[VAE] Epoch [36/50] Average Loss per Image: 351.9375\n",
            "[VAE][Epoch 37/50][Batch 0/391]: Loss per image: 362.9648\n",
            "[VAE][Epoch 37/50][Batch 100/391]: Loss per image: 372.0229\n",
            "[VAE][Epoch 37/50][Batch 200/391]: Loss per image: 362.6310\n",
            "[VAE][Epoch 37/50][Batch 300/391]: Loss per image: 343.0729\n",
            "[VAE] Epoch [37/50] Average Loss per Image: 350.9871\n",
            "[VAE][Epoch 38/50][Batch 0/391]: Loss per image: 353.2068\n",
            "[VAE][Epoch 38/50][Batch 100/391]: Loss per image: 340.2035\n",
            "[VAE][Epoch 38/50][Batch 200/391]: Loss per image: 357.6205\n",
            "[VAE][Epoch 38/50][Batch 300/391]: Loss per image: 353.6769\n",
            "[VAE] Epoch [38/50] Average Loss per Image: 350.3836\n",
            "[VAE][Epoch 39/50][Batch 0/391]: Loss per image: 340.7991\n",
            "[VAE][Epoch 39/50][Batch 100/391]: Loss per image: 350.8893\n",
            "[VAE][Epoch 39/50][Batch 200/391]: Loss per image: 345.5073\n",
            "[VAE][Epoch 39/50][Batch 300/391]: Loss per image: 349.5325\n",
            "[VAE] Epoch [39/50] Average Loss per Image: 349.0815\n",
            "[VAE][Epoch 40/50][Batch 0/391]: Loss per image: 350.5138\n",
            "[VAE][Epoch 40/50][Batch 100/391]: Loss per image: 341.2682\n",
            "[VAE][Epoch 40/50][Batch 200/391]: Loss per image: 336.3098\n",
            "[VAE][Epoch 40/50][Batch 300/391]: Loss per image: 348.9830\n",
            "[VAE] Epoch [40/50] Average Loss per Image: 348.9045\n",
            "[VAE][Epoch 41/50][Batch 0/391]: Loss per image: 340.3349\n",
            "[VAE][Epoch 41/50][Batch 100/391]: Loss per image: 346.1543\n",
            "[VAE][Epoch 41/50][Batch 200/391]: Loss per image: 339.6142\n",
            "[VAE][Epoch 41/50][Batch 300/391]: Loss per image: 353.0675\n",
            "[VAE] Epoch [41/50] Average Loss per Image: 348.4628\n",
            "[VAE][Epoch 42/50][Batch 0/391]: Loss per image: 354.1860\n",
            "[VAE][Epoch 42/50][Batch 100/391]: Loss per image: 348.0405\n",
            "[VAE][Epoch 42/50][Batch 200/391]: Loss per image: 358.1445\n",
            "[VAE][Epoch 42/50][Batch 300/391]: Loss per image: 369.4100\n",
            "[VAE] Epoch [42/50] Average Loss per Image: 347.6114\n",
            "[VAE][Epoch 43/50][Batch 0/391]: Loss per image: 338.5033\n",
            "[VAE][Epoch 43/50][Batch 100/391]: Loss per image: 338.5946\n",
            "[VAE][Epoch 43/50][Batch 200/391]: Loss per image: 341.9886\n",
            "[VAE][Epoch 43/50][Batch 300/391]: Loss per image: 349.2435\n",
            "[VAE] Epoch [43/50] Average Loss per Image: 347.4874\n",
            "[VAE][Epoch 44/50][Batch 0/391]: Loss per image: 335.8270\n",
            "[VAE][Epoch 44/50][Batch 100/391]: Loss per image: 351.5116\n",
            "[VAE][Epoch 44/50][Batch 200/391]: Loss per image: 353.5392\n",
            "[VAE][Epoch 44/50][Batch 300/391]: Loss per image: 354.3246\n",
            "[VAE] Epoch [44/50] Average Loss per Image: 346.7367\n",
            "[VAE][Epoch 45/50][Batch 0/391]: Loss per image: 346.5251\n",
            "[VAE][Epoch 45/50][Batch 100/391]: Loss per image: 333.3961\n",
            "[VAE][Epoch 45/50][Batch 200/391]: Loss per image: 327.2144\n",
            "[VAE][Epoch 45/50][Batch 300/391]: Loss per image: 349.1316\n",
            "[VAE] Epoch [45/50] Average Loss per Image: 346.2315\n",
            "[VAE][Epoch 46/50][Batch 0/391]: Loss per image: 342.0345\n",
            "[VAE][Epoch 46/50][Batch 100/391]: Loss per image: 346.6450\n",
            "[VAE][Epoch 46/50][Batch 200/391]: Loss per image: 364.0345\n",
            "[VAE][Epoch 46/50][Batch 300/391]: Loss per image: 358.7098\n",
            "[VAE] Epoch [46/50] Average Loss per Image: 345.2095\n",
            "[VAE][Epoch 47/50][Batch 0/391]: Loss per image: 343.2940\n",
            "[VAE][Epoch 47/50][Batch 100/391]: Loss per image: 347.3824\n",
            "[VAE][Epoch 47/50][Batch 200/391]: Loss per image: 349.0236\n",
            "[VAE][Epoch 47/50][Batch 300/391]: Loss per image: 347.2002\n",
            "[VAE] Epoch [47/50] Average Loss per Image: 345.7794\n",
            "[VAE][Epoch 48/50][Batch 0/391]: Loss per image: 338.1534\n",
            "[VAE][Epoch 48/50][Batch 100/391]: Loss per image: 349.1559\n",
            "[VAE][Epoch 48/50][Batch 200/391]: Loss per image: 360.8109\n",
            "[VAE][Epoch 48/50][Batch 300/391]: Loss per image: 344.2357\n",
            "[VAE] Epoch [48/50] Average Loss per Image: 344.3432\n",
            "[VAE][Epoch 49/50][Batch 0/391]: Loss per image: 335.0594\n",
            "[VAE][Epoch 49/50][Batch 100/391]: Loss per image: 355.0751\n",
            "[VAE][Epoch 49/50][Batch 200/391]: Loss per image: 342.8130\n",
            "[VAE][Epoch 49/50][Batch 300/391]: Loss per image: 348.2494\n",
            "[VAE] Epoch [49/50] Average Loss per Image: 344.8105\n",
            "[VAE][Epoch 50/50][Batch 0/391]: Loss per image: 351.6234\n",
            "[VAE][Epoch 50/50][Batch 100/391]: Loss per image: 346.9320\n",
            "[VAE][Epoch 50/50][Batch 200/391]: Loss per image: 344.4214\n",
            "[VAE][Epoch 50/50][Batch 300/391]: Loss per image: 318.9704\n",
            "[VAE] Epoch [50/50] Average Loss per Image: 344.2424\n",
            "VAE Training Complete.\n",
            "Starting Diffusion Model Training (Advanced Approach)...\n",
            "[Diffusion][Epoch 1/50][Batch 0/391]: Loss: 0.4605\n",
            "[Diffusion][Epoch 1/50][Batch 100/391]: Loss: 0.0042\n",
            "[Diffusion][Epoch 1/50][Batch 200/391]: Loss: 0.0028\n",
            "[Diffusion][Epoch 1/50][Batch 300/391]: Loss: 0.0036\n",
            "[Diffusion] Epoch [1/50] Average Loss: 0.0081\n",
            "[Diffusion][Epoch 2/50][Batch 0/391]: Loss: 0.0020\n",
            "[Diffusion][Epoch 2/50][Batch 100/391]: Loss: 0.0019\n",
            "[Diffusion][Epoch 2/50][Batch 200/391]: Loss: 0.0019\n",
            "[Diffusion][Epoch 2/50][Batch 300/391]: Loss: 0.0017\n",
            "[Diffusion] Epoch [2/50] Average Loss: 0.0023\n",
            "[Diffusion][Epoch 3/50][Batch 0/391]: Loss: 0.0019\n",
            "[Diffusion][Epoch 3/50][Batch 100/391]: Loss: 0.0013\n",
            "[Diffusion][Epoch 3/50][Batch 200/391]: Loss: 0.0015\n",
            "[Diffusion][Epoch 3/50][Batch 300/391]: Loss: 0.0015\n",
            "[Diffusion] Epoch [3/50] Average Loss: 0.0022\n",
            "[Diffusion][Epoch 4/50][Batch 0/391]: Loss: 0.0030\n",
            "[Diffusion][Epoch 4/50][Batch 100/391]: Loss: 0.0020\n",
            "[Diffusion][Epoch 4/50][Batch 200/391]: Loss: 0.0017\n",
            "[Diffusion][Epoch 4/50][Batch 300/391]: Loss: 0.0023\n",
            "[Diffusion] Epoch [4/50] Average Loss: 0.0021\n",
            "[Diffusion][Epoch 5/50][Batch 0/391]: Loss: 0.0026\n",
            "[Diffusion][Epoch 5/50][Batch 100/391]: Loss: 0.0019\n",
            "[Diffusion][Epoch 5/50][Batch 200/391]: Loss: 0.0029\n",
            "[Diffusion][Epoch 5/50][Batch 300/391]: Loss: 0.0020\n",
            "[Diffusion] Epoch [5/50] Average Loss: 0.0020\n",
            "[Diffusion][Epoch 6/50][Batch 0/391]: Loss: 0.0013\n",
            "[Diffusion][Epoch 6/50][Batch 100/391]: Loss: 0.0013\n",
            "[Diffusion][Epoch 6/50][Batch 200/391]: Loss: 0.0011\n",
            "[Diffusion][Epoch 6/50][Batch 300/391]: Loss: 0.0017\n",
            "[Diffusion] Epoch [6/50] Average Loss: 0.0019\n",
            "[Diffusion][Epoch 7/50][Batch 0/391]: Loss: 0.0019\n",
            "[Diffusion][Epoch 7/50][Batch 100/391]: Loss: 0.0019\n",
            "[Diffusion][Epoch 7/50][Batch 200/391]: Loss: 0.0014\n",
            "[Diffusion][Epoch 7/50][Batch 300/391]: Loss: 0.0012\n",
            "[Diffusion] Epoch [7/50] Average Loss: 0.0018\n",
            "[Diffusion][Epoch 8/50][Batch 0/391]: Loss: 0.0017\n",
            "[Diffusion][Epoch 8/50][Batch 100/391]: Loss: 0.0034\n",
            "[Diffusion][Epoch 8/50][Batch 200/391]: Loss: 0.0018\n",
            "[Diffusion][Epoch 8/50][Batch 300/391]: Loss: 0.0017\n",
            "[Diffusion] Epoch [8/50] Average Loss: 0.0018\n",
            "[Diffusion][Epoch 9/50][Batch 0/391]: Loss: 0.0011\n",
            "[Diffusion][Epoch 9/50][Batch 100/391]: Loss: 0.0023\n",
            "[Diffusion][Epoch 9/50][Batch 200/391]: Loss: 0.0015\n",
            "[Diffusion][Epoch 9/50][Batch 300/391]: Loss: 0.0021\n",
            "[Diffusion] Epoch [9/50] Average Loss: 0.0018\n",
            "[Diffusion][Epoch 10/50][Batch 0/391]: Loss: 0.0016\n",
            "[Diffusion][Epoch 10/50][Batch 100/391]: Loss: 0.0011\n",
            "[Diffusion][Epoch 10/50][Batch 200/391]: Loss: 0.0013\n",
            "[Diffusion][Epoch 10/50][Batch 300/391]: Loss: 0.0012\n",
            "[Diffusion] Epoch [10/50] Average Loss: 0.0018\n",
            "[Diffusion][Epoch 11/50][Batch 0/391]: Loss: 0.0035\n",
            "[Diffusion][Epoch 11/50][Batch 100/391]: Loss: 0.0010\n",
            "[Diffusion][Epoch 11/50][Batch 200/391]: Loss: 0.0011\n",
            "[Diffusion][Epoch 11/50][Batch 300/391]: Loss: 0.0019\n",
            "[Diffusion] Epoch [11/50] Average Loss: 0.0016\n",
            "[Diffusion][Epoch 12/50][Batch 0/391]: Loss: 0.0014\n",
            "[Diffusion][Epoch 12/50][Batch 100/391]: Loss: 0.0010\n",
            "[Diffusion][Epoch 12/50][Batch 200/391]: Loss: 0.0009\n",
            "[Diffusion][Epoch 12/50][Batch 300/391]: Loss: 0.0013\n",
            "[Diffusion] Epoch [12/50] Average Loss: 0.0016\n",
            "[Diffusion][Epoch 13/50][Batch 0/391]: Loss: 0.0010\n",
            "[Diffusion][Epoch 13/50][Batch 100/391]: Loss: 0.0012\n",
            "[Diffusion][Epoch 13/50][Batch 200/391]: Loss: 0.0044\n",
            "[Diffusion][Epoch 13/50][Batch 300/391]: Loss: 0.0022\n",
            "[Diffusion] Epoch [13/50] Average Loss: 0.0016\n",
            "[Diffusion][Epoch 14/50][Batch 0/391]: Loss: 0.0034\n",
            "[Diffusion][Epoch 14/50][Batch 100/391]: Loss: 0.0020\n",
            "[Diffusion][Epoch 14/50][Batch 200/391]: Loss: 0.0009\n",
            "[Diffusion][Epoch 14/50][Batch 300/391]: Loss: 0.0012\n",
            "[Diffusion] Epoch [14/50] Average Loss: 0.0017\n",
            "[Diffusion][Epoch 15/50][Batch 0/391]: Loss: 0.0009\n",
            "[Diffusion][Epoch 15/50][Batch 100/391]: Loss: 0.0012\n",
            "[Diffusion][Epoch 15/50][Batch 200/391]: Loss: 0.0013\n",
            "[Diffusion][Epoch 15/50][Batch 300/391]: Loss: 0.0016\n",
            "[Diffusion] Epoch [15/50] Average Loss: 0.0015\n",
            "[Diffusion][Epoch 16/50][Batch 0/391]: Loss: 0.0010\n",
            "[Diffusion][Epoch 16/50][Batch 100/391]: Loss: 0.0074\n",
            "[Diffusion][Epoch 16/50][Batch 200/391]: Loss: 0.0011\n",
            "[Diffusion][Epoch 16/50][Batch 300/391]: Loss: 0.0021\n",
            "[Diffusion] Epoch [16/50] Average Loss: 0.0015\n",
            "[Diffusion][Epoch 17/50][Batch 0/391]: Loss: 0.0056\n",
            "[Diffusion][Epoch 17/50][Batch 100/391]: Loss: 0.0011\n",
            "[Diffusion][Epoch 17/50][Batch 200/391]: Loss: 0.0015\n",
            "[Diffusion][Epoch 17/50][Batch 300/391]: Loss: 0.0021\n",
            "[Diffusion] Epoch [17/50] Average Loss: 0.0016\n",
            "[Diffusion][Epoch 18/50][Batch 0/391]: Loss: 0.0014\n",
            "[Diffusion][Epoch 18/50][Batch 100/391]: Loss: 0.0017\n",
            "[Diffusion][Epoch 18/50][Batch 200/391]: Loss: 0.0016\n",
            "[Diffusion][Epoch 18/50][Batch 300/391]: Loss: 0.0009\n",
            "[Diffusion] Epoch [18/50] Average Loss: 0.0016\n",
            "[Diffusion][Epoch 19/50][Batch 0/391]: Loss: 0.0057\n",
            "[Diffusion][Epoch 19/50][Batch 100/391]: Loss: 0.0009\n",
            "[Diffusion][Epoch 19/50][Batch 200/391]: Loss: 0.0010\n",
            "[Diffusion][Epoch 19/50][Batch 300/391]: Loss: 0.0013\n",
            "[Diffusion] Epoch [19/50] Average Loss: 0.0016\n",
            "[Diffusion][Epoch 20/50][Batch 0/391]: Loss: 0.0017\n",
            "[Diffusion][Epoch 20/50][Batch 100/391]: Loss: 0.0009\n",
            "[Diffusion][Epoch 20/50][Batch 200/391]: Loss: 0.0018\n",
            "[Diffusion][Epoch 20/50][Batch 300/391]: Loss: 0.0012\n",
            "[Diffusion] Epoch [20/50] Average Loss: 0.0015\n",
            "[Diffusion][Epoch 21/50][Batch 0/391]: Loss: 0.0011\n",
            "[Diffusion][Epoch 21/50][Batch 100/391]: Loss: 0.0022\n",
            "[Diffusion][Epoch 21/50][Batch 200/391]: Loss: 0.0015\n",
            "[Diffusion][Epoch 21/50][Batch 300/391]: Loss: 0.0011\n",
            "[Diffusion] Epoch [21/50] Average Loss: 0.0014\n",
            "[Diffusion][Epoch 22/50][Batch 0/391]: Loss: 0.0018\n",
            "[Diffusion][Epoch 22/50][Batch 100/391]: Loss: 0.0017\n",
            "[Diffusion][Epoch 22/50][Batch 200/391]: Loss: 0.0013\n",
            "[Diffusion][Epoch 22/50][Batch 300/391]: Loss: 0.0009\n",
            "[Diffusion] Epoch [22/50] Average Loss: 0.0015\n",
            "[Diffusion][Epoch 23/50][Batch 0/391]: Loss: 0.0015\n",
            "[Diffusion][Epoch 23/50][Batch 100/391]: Loss: 0.0017\n",
            "[Diffusion][Epoch 23/50][Batch 200/391]: Loss: 0.0022\n",
            "[Diffusion][Epoch 23/50][Batch 300/391]: Loss: 0.0014\n",
            "[Diffusion] Epoch [23/50] Average Loss: 0.0016\n",
            "[Diffusion][Epoch 24/50][Batch 0/391]: Loss: 0.0015\n",
            "[Diffusion][Epoch 24/50][Batch 100/391]: Loss: 0.0027\n",
            "[Diffusion][Epoch 24/50][Batch 200/391]: Loss: 0.0025\n",
            "[Diffusion][Epoch 24/50][Batch 300/391]: Loss: 0.0028\n",
            "[Diffusion] Epoch [24/50] Average Loss: 0.0015\n",
            "[Diffusion][Epoch 25/50][Batch 0/391]: Loss: 0.0009\n",
            "[Diffusion][Epoch 25/50][Batch 100/391]: Loss: 0.0012\n",
            "[Diffusion][Epoch 25/50][Batch 200/391]: Loss: 0.0028\n",
            "[Diffusion][Epoch 25/50][Batch 300/391]: Loss: 0.0011\n",
            "[Diffusion] Epoch [25/50] Average Loss: 0.0014\n",
            "[Diffusion][Epoch 26/50][Batch 0/391]: Loss: 0.0009\n",
            "[Diffusion][Epoch 26/50][Batch 100/391]: Loss: 0.0008\n",
            "[Diffusion][Epoch 26/50][Batch 200/391]: Loss: 0.0014\n",
            "[Diffusion][Epoch 26/50][Batch 300/391]: Loss: 0.0014\n",
            "[Diffusion] Epoch [26/50] Average Loss: 0.0014\n",
            "[Diffusion][Epoch 27/50][Batch 0/391]: Loss: 0.0014\n",
            "[Diffusion][Epoch 27/50][Batch 100/391]: Loss: 0.0008\n",
            "[Diffusion][Epoch 27/50][Batch 200/391]: Loss: 0.0017\n",
            "[Diffusion][Epoch 27/50][Batch 300/391]: Loss: 0.0016\n",
            "[Diffusion] Epoch [27/50] Average Loss: 0.0014\n",
            "[Diffusion][Epoch 28/50][Batch 0/391]: Loss: 0.0014\n",
            "[Diffusion][Epoch 28/50][Batch 100/391]: Loss: 0.0010\n",
            "[Diffusion][Epoch 28/50][Batch 200/391]: Loss: 0.0023\n",
            "[Diffusion][Epoch 28/50][Batch 300/391]: Loss: 0.0018\n",
            "[Diffusion] Epoch [28/50] Average Loss: 0.0014\n",
            "[Diffusion][Epoch 29/50][Batch 0/391]: Loss: 0.0012\n",
            "[Diffusion][Epoch 29/50][Batch 100/391]: Loss: 0.0016\n",
            "[Diffusion][Epoch 29/50][Batch 200/391]: Loss: 0.0012\n",
            "[Diffusion][Epoch 29/50][Batch 300/391]: Loss: 0.0019\n",
            "[Diffusion] Epoch [29/50] Average Loss: 0.0013\n",
            "[Diffusion][Epoch 30/50][Batch 0/391]: Loss: 0.0010\n",
            "[Diffusion][Epoch 30/50][Batch 100/391]: Loss: 0.0026\n",
            "[Diffusion][Epoch 30/50][Batch 200/391]: Loss: 0.0011\n",
            "[Diffusion][Epoch 30/50][Batch 300/391]: Loss: 0.0015\n",
            "[Diffusion] Epoch [30/50] Average Loss: 0.0014\n",
            "[Diffusion][Epoch 31/50][Batch 0/391]: Loss: 0.0016\n",
            "[Diffusion][Epoch 31/50][Batch 100/391]: Loss: 0.0012\n",
            "[Diffusion][Epoch 31/50][Batch 200/391]: Loss: 0.0009\n",
            "[Diffusion][Epoch 31/50][Batch 300/391]: Loss: 0.0010\n",
            "[Diffusion] Epoch [31/50] Average Loss: 0.0013\n",
            "[Diffusion][Epoch 32/50][Batch 0/391]: Loss: 0.0011\n",
            "[Diffusion][Epoch 32/50][Batch 100/391]: Loss: 0.0010\n",
            "[Diffusion][Epoch 32/50][Batch 200/391]: Loss: 0.0010\n",
            "[Diffusion][Epoch 32/50][Batch 300/391]: Loss: 0.0013\n",
            "[Diffusion] Epoch [32/50] Average Loss: 0.0013\n",
            "[Diffusion][Epoch 33/50][Batch 0/391]: Loss: 0.0008\n",
            "[Diffusion][Epoch 33/50][Batch 100/391]: Loss: 0.0012\n",
            "[Diffusion][Epoch 33/50][Batch 200/391]: Loss: 0.0010\n",
            "[Diffusion][Epoch 33/50][Batch 300/391]: Loss: 0.0011\n",
            "[Diffusion] Epoch [33/50] Average Loss: 0.0012\n",
            "[Diffusion][Epoch 34/50][Batch 0/391]: Loss: 0.0010\n",
            "[Diffusion][Epoch 34/50][Batch 100/391]: Loss: 0.0011\n",
            "[Diffusion][Epoch 34/50][Batch 200/391]: Loss: 0.0010\n",
            "[Diffusion][Epoch 34/50][Batch 300/391]: Loss: 0.0011\n",
            "[Diffusion] Epoch [34/50] Average Loss: 0.0013\n",
            "[Diffusion][Epoch 35/50][Batch 0/391]: Loss: 0.0010\n",
            "[Diffusion][Epoch 35/50][Batch 100/391]: Loss: 0.0013\n",
            "[Diffusion][Epoch 35/50][Batch 200/391]: Loss: 0.0009\n",
            "[Diffusion][Epoch 35/50][Batch 300/391]: Loss: 0.0018\n",
            "[Diffusion] Epoch [35/50] Average Loss: 0.0012\n",
            "[Diffusion][Epoch 36/50][Batch 0/391]: Loss: 0.0012\n",
            "[Diffusion][Epoch 36/50][Batch 100/391]: Loss: 0.0021\n",
            "[Diffusion][Epoch 36/50][Batch 200/391]: Loss: 0.0015\n",
            "[Diffusion][Epoch 36/50][Batch 300/391]: Loss: 0.0009\n",
            "[Diffusion] Epoch [36/50] Average Loss: 0.0011\n",
            "[Diffusion][Epoch 37/50][Batch 0/391]: Loss: 0.0015\n",
            "[Diffusion][Epoch 37/50][Batch 100/391]: Loss: 0.0012\n",
            "[Diffusion][Epoch 37/50][Batch 200/391]: Loss: 0.0012\n",
            "[Diffusion][Epoch 37/50][Batch 300/391]: Loss: 0.0013\n",
            "[Diffusion] Epoch [37/50] Average Loss: 0.0012\n",
            "[Diffusion][Epoch 38/50][Batch 0/391]: Loss: 0.0008\n",
            "[Diffusion][Epoch 38/50][Batch 100/391]: Loss: 0.0016\n",
            "[Diffusion][Epoch 38/50][Batch 200/391]: Loss: 0.0010\n",
            "[Diffusion][Epoch 38/50][Batch 300/391]: Loss: 0.0026\n",
            "[Diffusion] Epoch [38/50] Average Loss: 0.0012\n",
            "[Diffusion][Epoch 39/50][Batch 0/391]: Loss: 0.0012\n",
            "[Diffusion][Epoch 39/50][Batch 100/391]: Loss: 0.0011\n",
            "[Diffusion][Epoch 39/50][Batch 200/391]: Loss: 0.0009\n",
            "[Diffusion][Epoch 39/50][Batch 300/391]: Loss: 0.0009\n",
            "[Diffusion] Epoch [39/50] Average Loss: 0.0012\n",
            "[Diffusion][Epoch 40/50][Batch 0/391]: Loss: 0.0008\n",
            "[Diffusion][Epoch 40/50][Batch 100/391]: Loss: 0.0010\n",
            "[Diffusion][Epoch 40/50][Batch 200/391]: Loss: 0.0011\n",
            "[Diffusion][Epoch 40/50][Batch 300/391]: Loss: 0.0010\n",
            "[Diffusion] Epoch [40/50] Average Loss: 0.0011\n",
            "[Diffusion][Epoch 41/50][Batch 0/391]: Loss: 0.0019\n",
            "[Diffusion][Epoch 41/50][Batch 100/391]: Loss: 0.0009\n",
            "[Diffusion][Epoch 41/50][Batch 200/391]: Loss: 0.0008\n",
            "[Diffusion][Epoch 41/50][Batch 300/391]: Loss: 0.0009\n",
            "[Diffusion] Epoch [41/50] Average Loss: 0.0011\n",
            "[Diffusion][Epoch 42/50][Batch 0/391]: Loss: 0.0010\n",
            "[Diffusion][Epoch 42/50][Batch 100/391]: Loss: 0.0014\n",
            "[Diffusion][Epoch 42/50][Batch 200/391]: Loss: 0.0010\n",
            "[Diffusion][Epoch 42/50][Batch 300/391]: Loss: 0.0011\n",
            "[Diffusion] Epoch [42/50] Average Loss: 0.0011\n",
            "[Diffusion][Epoch 43/50][Batch 0/391]: Loss: 0.0008\n",
            "[Diffusion][Epoch 43/50][Batch 100/391]: Loss: 0.0010\n",
            "[Diffusion][Epoch 43/50][Batch 200/391]: Loss: 0.0010\n",
            "[Diffusion][Epoch 43/50][Batch 300/391]: Loss: 0.0010\n",
            "[Diffusion] Epoch [43/50] Average Loss: 0.0010\n",
            "[Diffusion][Epoch 44/50][Batch 0/391]: Loss: 0.0009\n",
            "[Diffusion][Epoch 44/50][Batch 100/391]: Loss: 0.0009\n",
            "[Diffusion][Epoch 44/50][Batch 200/391]: Loss: 0.0008\n",
            "[Diffusion][Epoch 44/50][Batch 300/391]: Loss: 0.0008\n",
            "[Diffusion] Epoch [44/50] Average Loss: 0.0011\n",
            "[Diffusion][Epoch 45/50][Batch 0/391]: Loss: 0.0008\n",
            "[Diffusion][Epoch 45/50][Batch 100/391]: Loss: 0.0008\n",
            "[Diffusion][Epoch 45/50][Batch 200/391]: Loss: 0.0009\n",
            "[Diffusion][Epoch 45/50][Batch 300/391]: Loss: 0.0009\n",
            "[Diffusion] Epoch [45/50] Average Loss: 0.0010\n",
            "[Diffusion][Epoch 46/50][Batch 0/391]: Loss: 0.0010\n",
            "[Diffusion][Epoch 46/50][Batch 100/391]: Loss: 0.0011\n",
            "[Diffusion][Epoch 46/50][Batch 200/391]: Loss: 0.0009\n",
            "[Diffusion][Epoch 46/50][Batch 300/391]: Loss: 0.0010\n",
            "[Diffusion] Epoch [46/50] Average Loss: 0.0010\n",
            "[Diffusion][Epoch 47/50][Batch 0/391]: Loss: 0.0008\n",
            "[Diffusion][Epoch 47/50][Batch 100/391]: Loss: 0.0014\n",
            "[Diffusion][Epoch 47/50][Batch 200/391]: Loss: 0.0010\n",
            "[Diffusion][Epoch 47/50][Batch 300/391]: Loss: 0.0010\n",
            "[Diffusion] Epoch [47/50] Average Loss: 0.0010\n",
            "[Diffusion][Epoch 48/50][Batch 0/391]: Loss: 0.0011\n",
            "[Diffusion][Epoch 48/50][Batch 100/391]: Loss: 0.0008\n",
            "[Diffusion][Epoch 48/50][Batch 200/391]: Loss: 0.0011\n",
            "[Diffusion][Epoch 48/50][Batch 300/391]: Loss: 0.0008\n",
            "[Diffusion] Epoch [48/50] Average Loss: 0.0010\n",
            "[Diffusion][Epoch 49/50][Batch 0/391]: Loss: 0.0009\n",
            "[Diffusion][Epoch 49/50][Batch 100/391]: Loss: 0.0013\n",
            "[Diffusion][Epoch 49/50][Batch 200/391]: Loss: 0.0009\n",
            "[Diffusion][Epoch 49/50][Batch 300/391]: Loss: 0.0012\n",
            "[Diffusion] Epoch [49/50] Average Loss: 0.0009\n",
            "[Diffusion][Epoch 50/50][Batch 0/391]: Loss: 0.0009\n",
            "[Diffusion][Epoch 50/50][Batch 100/391]: Loss: 0.0009\n",
            "[Diffusion][Epoch 50/50][Batch 200/391]: Loss: 0.0012\n",
            "[Diffusion][Epoch 50/50][Batch 300/391]: Loss: 0.0011\n",
            "[Diffusion] Epoch [50/50] Average Loss: 0.0009\n",
            "Diffusion Model Training Complete.\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python\n",
        "\"\"\"\n",
        "Project Title: Benchmarking Generative Models for Image Synthesis and Enhancement in Computer Vision\n",
        "\n",
        "===============================================================================\n",
        "1. Project Type\n",
        "===============================================================================\n",
        "This project reproduces baseline implementations of three generative models –\n",
        "Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and\n",
        "Diffusion Models – using custom code. The goal is to train and test these models\n",
        "on publicly available datasets (e.g., CIFAR-10) and benchmark them with standard\n",
        "metrics such as FID and Inception Score.\n",
        "\n",
        "===============================================================================\n",
        "2. Project Introduction\n",
        "===============================================================================\n",
        "In this project we implement three types of generative models from scratch:\n",
        "• A GAN (DCGAN architecture) for adversarial image synthesis.\n",
        "• A VAE for probabilistic latent space encoding.\n",
        "• A Diffusion Model (with an advanced approach including attention mechanisms)\n",
        "  for iterative image denoising.\n",
        "We will train these models on the CIFAR-10 dataset (as chosen by the user) and\n",
        "compare their performance both quantitatively (using FID/IS) and qualitatively.\n",
        "Refer to [1] for related state-of-the-art references.\n",
        "\n",
        "===============================================================================\n",
        "3. Project Motivation\n",
        "===============================================================================\n",
        "Benchmarking multiple generative models on CIFAR-10 will provide a deep\n",
        "understanding of the computational trade-offs and architectural differences\n",
        "between GANs, VAEs, and Diffusion Models. In addition, incorporating attention\n",
        "mechanisms in the diffusion model offers insights into improving image quality\n",
        "and diversity.\n",
        "\n",
        "===============================================================================\n",
        "4. Project Plan\n",
        "===============================================================================\n",
        "4.1. Baseline Approach:\n",
        "     - Implement baseline GAN (DCGAN), VAE, and a simple Diffusion Model.\n",
        "     - Train using CIFAR-10 images (upscaled to 64x64 if necessary).\n",
        "\n",
        "4.2. Advanced Approach:\n",
        "     - Enhance the diffusion model with integrated self-attention layers.\n",
        "\n",
        "4.3. Validation Plan:\n",
        "     - Evaluate generated images using FID and Inception Score.\n",
        "     - Perform qualitative visual assessments on saved image outputs.\n",
        "\n",
        "4.4. Computational Resources:\n",
        "     - Use Python with PyTorch, running on GPU if available.\n",
        "\n",
        "4.5. Libraries/Tools:\n",
        "     - Python, PyTorch, torchvision, matplotlib, OpenCV, scikit-learn.\n",
        "\n",
        "4.6. Estimated Baseline Runtime:\n",
        "     - Expect around 1-2 hours per baseline model for training; evaluation may\n",
        "       take a full day depending on cross-validation and experimental runs.\n",
        "\n",
        "===============================================================================\n",
        "5. Workload\n",
        "===============================================================================\n",
        "The project involves extensive implementation from scratch, hyperparameter\n",
        "tuning, literature review, and experiments comparing different models.\n",
        "\n",
        "===============================================================================\n",
        "6. Ideal Result & Insights\n",
        "===============================================================================\n",
        "The ultimate outcome is a comprehensive benchmarking report and a novel diffusion\n",
        "model variant leveraging attention that demonstrates improved performance.\n",
        "Insights will cover computational trade-offs and model-specific strengths.\n",
        "\n",
        "===============================================================================\n",
        "7. Potential Risks\n",
        "===============================================================================\n",
        "Potential challenges include model instability, insufficient GPU resources,\n",
        "and difficulty matching the performance of highly optimized existing codebases.\n",
        "\n",
        "===============================================================================\n",
        "8. Duplication Statement\n",
        "===============================================================================\n",
        "This project is an original effort with no duplication of prior research work.\n",
        "\n",
        "===============================================================================\n",
        "References:\n",
        "[1] Maxime Oquab, Timothée Darcet, Théo Moutakanni, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023.\n",
        "===============================================================================\n",
        "\"\"\"\n",
        "!pip install pytorch-gan-metrics\n",
        "# ===========================\n",
        "# Section 1: Configuration and Setup\n",
        "# ===========================\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, utils as vutils\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility.\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Hyperparameters and configuration.\n",
        "DATA_PATH     = './data'\n",
        "BATCH_SIZE    = 128\n",
        "IMAGE_SIZE    = 64            # Upscale CIFAR-10 from 32x32 to 64x64 for compatibility.\n",
        "NOISE_DIM     = 100           # Dimension of noise vector for GAN.\n",
        "NUM_EPOCHS    = 50            # Number of training epochs (modify as needed).\n",
        "LR            = 0.0002\n",
        "BETA1         = 0.5\n",
        "CHANNELS_IMG  = 3             # CIFAR-10 images are RGB.\n",
        "OUTPUT_DIR    = './output'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ===========================\n",
        "# Section 2: Data Loading & Preprocessing (CIFAR-10)\n",
        "# ===========================\n",
        "def get_cifar10_dataloader(data_path=DATA_PATH, batch_size=BATCH_SIZE, image_size=IMAGE_SIZE):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(image_size),                              # Resize images to 64x64.\n",
        "        transforms.ToTensor(),                                      # Convert to PyTorch tensors.\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))      # Scale to [-1, 1].\n",
        "    ])\n",
        "    dataset = datasets.CIFAR10(root=data_path, train=True, transform=transform, download=True)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "# ===========================\n",
        "# Section 3: Baseline Approaches\n",
        "# ---------------------------\n",
        "# 3.1. Baseline GAN (DCGAN)\n",
        "# ===========================\n",
        "class GANGenerator(nn.Module):\n",
        "    def __init__(self, noise_dim=NOISE_DIM, feature_map_size=64, channels=CHANNELS_IMG):\n",
        "        super(GANGenerator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            # Input: latent vector Z reshaped as (noise_dim x 1 x 1)\n",
        "            nn.ConvTranspose2d(noise_dim, feature_map_size * 8, kernel_size=4, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(feature_map_size * 8),\n",
        "            nn.ReLU(True),\n",
        "            # State: (feature_map_size*8) x 4 x 4\n",
        "            nn.ConvTranspose2d(feature_map_size * 8, feature_map_size * 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(feature_map_size * 4),\n",
        "            nn.ReLU(True),\n",
        "            # State: (feature_map_size*4) x 8 x 8\n",
        "            nn.ConvTranspose2d(feature_map_size * 4, feature_map_size * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(feature_map_size * 2),\n",
        "            nn.ReLU(True),\n",
        "            # State: (feature_map_size*2) x 16 x 16\n",
        "            nn.ConvTranspose2d(feature_map_size * 2, feature_map_size, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(feature_map_size),\n",
        "            nn.ReLU(True),\n",
        "            # State: (feature_map_size) x 32 x 32\n",
        "            nn.ConvTranspose2d(feature_map_size, channels, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.Tanh()  # Output scaled to [-1, 1]\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class GANDiscriminator(nn.Module):\n",
        "    def __init__(self, feature_map_size=64, channels=CHANNELS_IMG):\n",
        "        super(GANDiscriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            # Input: (channels) x 64 x 64\n",
        "            nn.Conv2d(channels, feature_map_size, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(feature_map_size, feature_map_size * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(feature_map_size * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(feature_map_size * 2, feature_map_size * 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(feature_map_size * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(feature_map_size * 4, feature_map_size * 8, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(feature_map_size * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # Final layer: Output a single probability value (real/fake)\n",
        "            nn.Conv2d(feature_map_size * 8, 1, kernel_size=4, stride=1, padding=0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# ---------------------------\n",
        "# 3.2. Baseline VAE\n",
        "# ---------------------------\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, img_channels=CHANNELS_IMG, latent_dim=128, feature_dim=64):\n",
        "        super(VAE, self).__init__()\n",
        "        # Encoder: Convolutional layers to extract features.\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(img_channels, feature_dim, kernel_size=4, stride=2, padding=1),  # 64 -> 32\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(feature_dim, feature_dim*2, kernel_size=4, stride=2, padding=1),   # 32 -> 16\n",
        "            nn.BatchNorm2d(feature_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(feature_dim*2, feature_dim*4, kernel_size=4, stride=2, padding=1),   # 16 -> 8\n",
        "            nn.BatchNorm2d(feature_dim*4),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten()   # Flatten for the FC layers.\n",
        "        )\n",
        "        # Calculate flattened feature size assuming input image size 64x64.\n",
        "        self.fc_mu    = nn.Linear(feature_dim*4*8*8, latent_dim)\n",
        "        self.fc_logvar= nn.Linear(feature_dim*4*8*8, latent_dim)\n",
        "        # Decoder: Map latent vector back to image space.\n",
        "        self.decoder_input = nn.Linear(latent_dim, feature_dim*4*8*8)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Unflatten(1, (feature_dim*4, 8, 8)),\n",
        "            nn.ConvTranspose2d(feature_dim*4, feature_dim*2, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(feature_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(feature_dim*2, feature_dim, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(feature_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(feature_dim, img_channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu = self.fc_mu(h)\n",
        "        logvar = self.fc_logvar(h)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h = self.decoder_input(z)\n",
        "        return self.decoder(h)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        x_recon = self.decode(z)\n",
        "        return x_recon, mu, logvar\n",
        "\n",
        "# ---------------------------\n",
        "# 3.3. Baseline Diffusion Model (Advanced Approach) with Integrated Attention\n",
        "# ---------------------------\n",
        "# Self-Attention Module:\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.query = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
        "        self.key   = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
        "        self.value = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "    def forward(self, x):\n",
        "        b, c, w, h = x.size()\n",
        "        proj_query = self.query(x).view(b, -1, w*h).permute(0, 2, 1)\n",
        "        proj_key   = self.key(x).view(b, -1, w*h)\n",
        "        energy = torch.bmm(proj_query, proj_key)\n",
        "        attention = self.softmax(energy)\n",
        "        proj_value = self.value(x).view(b, -1, w*h)\n",
        "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
        "        out = out.view(b, c, w, h)\n",
        "        out = self.gamma * out + x\n",
        "        return out\n",
        "\n",
        "# UNet Block with optional attention\n",
        "class UNetBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, use_attention=False):\n",
        "        super(UNetBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.bn1   = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.bn2   = nn.BatchNorm2d(out_channels)\n",
        "        self.relu  = nn.ReLU(inplace=True)\n",
        "        self.use_attention = use_attention\n",
        "        if use_attention:\n",
        "            self.attn = SelfAttention(out_channels)\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        if self.use_attention:\n",
        "            x = self.attn(x)\n",
        "        return x\n",
        "\n",
        "# Simplified Diffusion Model (U-Net Style)\n",
        "class DiffusionModel(nn.Module):\n",
        "    def __init__(self, img_channels=CHANNELS_IMG, feature_maps=64):\n",
        "        super(DiffusionModel, self).__init__()\n",
        "        # Down-sampling path.\n",
        "        self.down1 = UNetBlock(img_channels, feature_maps, use_attention=False)\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.down2 = UNetBlock(feature_maps, feature_maps*2, use_attention=True)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        # Bottleneck with attention.\n",
        "        self.bottleneck = UNetBlock(feature_maps*2, feature_maps*4, use_attention=True)\n",
        "        # Up-sampling path with skip connections.\n",
        "        self.up1 = nn.ConvTranspose2d(feature_maps*4, feature_maps*2, kernel_size=2, stride=2)\n",
        "        self.up_block1 = UNetBlock(feature_maps*4, feature_maps*2, use_attention=True)\n",
        "        self.up2 = nn.ConvTranspose2d(feature_maps*2, feature_maps, kernel_size=2, stride=2)\n",
        "        self.up_block2 = UNetBlock(feature_maps*2, feature_maps, use_attention=False)\n",
        "        self.final_conv = nn.Conv2d(feature_maps, img_channels, kernel_size=1)\n",
        "    def forward(self, x, t=None):\n",
        "        # 't' represents the timestep for the diffusion process (optional embedding).\n",
        "        d1 = self.down1(x)\n",
        "        p1 = self.pool1(d1)\n",
        "        d2 = self.down2(p1)\n",
        "        p2 = self.pool2(d2)\n",
        "        bn = self.bottleneck(p2)\n",
        "        u1 = self.up1(bn)\n",
        "        # First skip connection.\n",
        "        u1 = torch.cat([u1, d2], dim=1)\n",
        "        u1 = self.up_block1(u1)\n",
        "        u2 = self.up2(u1)\n",
        "        # Second skip connection.\n",
        "        u2 = torch.cat([u2, d1], dim=1)\n",
        "        u2 = self.up_block2(u2)\n",
        "        out = self.final_conv(u2)\n",
        "        return out\n",
        "\n",
        "# ===========================\n",
        "# Section 4: Validation Plan (Evaluation and Metrics)\n",
        "# ===========================\n",
        "import os\n",
        "import torch\n",
        "from torchvision import utils as vutils\n",
        "\n",
        "def evaluate_generated_images(generator, fixed_noise, output_dir=OUTPUT_DIR, epoch=0, stats_path='path/to/statistics.npz'):\n",
        "    \"\"\"\n",
        "    Generate images using the fixed noise vector, save a composite grid,\n",
        "    and compute Inception Score (IS) and Frechet Inception Distance (FID)\n",
        "    using the pytorch-gan-metrics package.\n",
        "\n",
        "    Parameters:\n",
        "      generator: Trained generator model.\n",
        "      fixed_noise: A fixed noise tensor (e.g., [N, noise_dim, 1, 1]).\n",
        "      output_dir: Directory for saving images.\n",
        "      epoch: Current epoch number (used in filenames).\n",
        "      stats_path: Path to the NPZ file containing FID statistics for real images.\n",
        "                  This file must be prepared using the package command line tool.\n",
        "\n",
        "    Requirements:\n",
        "      pip install pytorch-gan-metrics\n",
        "\n",
        "    Note:\n",
        "      The generator is assumed to produce images in the range [-1, 1]. They\n",
        "      are converted to [0, 1] before metric calculation.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate fake images.\n",
        "    with torch.no_grad():\n",
        "        fake_images = generator(fixed_noise).detach().cpu()\n",
        "\n",
        "    # Save composite image grid (normalized for visualization).\n",
        "    composite_filename = os.path.join(output_dir, f\"gan_epoch_{epoch:03d}.png\")\n",
        "    vutils.save_image(fake_images, composite_filename, normalize=True)\n",
        "    print(f\"Saved composite generated images to {composite_filename}\")\n",
        "\n",
        "    # Convert images from [-1, 1] to [0, 1] for the metric calculations.\n",
        "    fake_images_01 = (fake_images + 1) / 2.0\n",
        "\n",
        "    # Import metric functions from pytorch-gan-metrics.\n",
        "    try:\n",
        "        from pytorch_gan_metrics import get_inception_score, get_fid\n",
        "    except ImportError:\n",
        "        print(\"Please install pytorch-gan-metrics via 'pip install pytorch-gan-metrics'\")\n",
        "        return\n",
        "\n",
        "    # Compute Inception Score.\n",
        "    try:\n",
        "        IS, IS_std = get_inception_score(fake_images_01, batch_size=32, resize=True, splits=10)\n",
        "        print(f\"Inception Score: {IS:.2f} ± {IS_std:.2f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error computing Inception Score: {e}\")\n",
        "\n",
        "    # Compute FID Score if the statistics file exists.\n",
        "    if not os.path.exists(stats_path):\n",
        "        print(f\"Statistics file not found at {stats_path}. Cannot compute FID.\")\n",
        "    else:\n",
        "        try:\n",
        "            FID = get_fid(fake_images_01, stats_path)\n",
        "            print(f\"FID: {FID:.2f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error computing FID: {e}\")\n",
        "\n",
        "\n",
        "# ===========================\n",
        "# Section 5: Training Utilities and Main Execution\n",
        "# ===========================\n",
        "def train_gan(dataloader, num_epochs=NUM_EPOCHS):\n",
        "    # Initialize baseline GAN models.\n",
        "    netG = GANGenerator().to(device)\n",
        "    netD = GANDiscriminator().to(device)\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizerD = optim.Adam(netD.parameters(), lr=LR, betas=(BETA1, 0.999))\n",
        "    optimizerG = optim.Adam(netG.parameters(), lr=LR, betas=(BETA1, 0.999))\n",
        "    fixed_noise = torch.randn(64, NOISE_DIM, 1, 1, device=device)\n",
        "\n",
        "    print(\"Starting GAN Training (Baseline Approach)...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (real_data, _) in enumerate(dataloader):\n",
        "            # ------------------------------\n",
        "            # (a) Update Discriminator:\n",
        "            # Maximize log(D(x)) + log(1-D(G(z)))\n",
        "            # ------------------------------\n",
        "            netD.zero_grad()\n",
        "            real_data = real_data.to(device)\n",
        "            b_size = real_data.size(0)\n",
        "            label_real = torch.full((b_size,), 1.0, device=device)\n",
        "            output_real = netD(real_data).view(-1)\n",
        "            lossD_real = criterion(output_real, label_real)\n",
        "            lossD_real.backward()\n",
        "\n",
        "            noise = torch.randn(b_size, NOISE_DIM, 1, 1, device=device)\n",
        "            fake_data = netG(noise)\n",
        "            label_fake = torch.full((b_size,), 0.0, device=device)\n",
        "            output_fake = netD(fake_data.detach()).view(-1)\n",
        "            lossD_fake = criterion(output_fake, label_fake)\n",
        "            lossD_fake.backward()\n",
        "            lossD = lossD_real + lossD_fake\n",
        "            optimizerD.step()\n",
        "\n",
        "            # ------------------------------\n",
        "            # (b) Update Generator:\n",
        "            # Maximize log(D(G(z))) by \"tricking\" the discriminator.\n",
        "            # ------------------------------\n",
        "            netG.zero_grad()\n",
        "            label_gen = torch.full((b_size,), 1.0, device=device)\n",
        "            output_gen = netD(fake_data).view(-1)\n",
        "            lossG = criterion(output_gen, label_gen)\n",
        "            lossG.backward()\n",
        "            optimizerG.step()\n",
        "\n",
        "            if i % 100 == 0:\n",
        "                print(f\"[Epoch {epoch+1}/{num_epochs}][Batch {i}/{len(dataloader)}]: Loss_D: {lossD.item():.4f}, Loss_G: {lossG.item():.4f}\")\n",
        "\n",
        "        # Evaluate and save generated images at the end of each epoch.\n",
        "        evaluate_generated_images(netG, fixed_noise, epoch=epoch+1)\n",
        "    print(\"GAN Training Complete.\")\n",
        "\n",
        "def train_vae(dataloader, num_epochs=NUM_EPOCHS):\n",
        "    print(\"Starting VAE Training (Baseline Approach)...\")\n",
        "    vae = VAE().to(device)\n",
        "    optimizerVAE = optim.Adam(vae.parameters(), lr=LR)\n",
        "    reconstruction_loss_fn = nn.MSELoss(reduction='sum')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, (data, _) in enumerate(dataloader):\n",
        "            vae.zero_grad()\n",
        "            data = data.to(device)\n",
        "            x_recon, mu, logvar = vae(data)\n",
        "            recon_loss = reconstruction_loss_fn(x_recon, data)\n",
        "            # KL Divergence Loss.\n",
        "            kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "            loss = recon_loss + kl_loss\n",
        "            loss.backward()\n",
        "            optimizerVAE.step()\n",
        "            running_loss += loss.item()\n",
        "            if i % 100 == 0:\n",
        "                avg_loss = loss.item() / data.size(0)\n",
        "                print(f\"[VAE][Epoch {epoch+1}/{num_epochs}][Batch {i}/{len(dataloader)}]: Loss per image: {avg_loss:.4f}\")\n",
        "        epoch_loss = running_loss / len(dataloader.dataset)\n",
        "        print(f\"[VAE] Epoch [{epoch+1}/{num_epochs}] Average Loss per Image: {epoch_loss:.4f}\")\n",
        "\n",
        "        # Evaluate: reconstruct a fixed batch and save.\n",
        "        with torch.no_grad():\n",
        "            sample_batch, _ = next(iter(dataloader))\n",
        "            sample_batch = sample_batch.to(device)\n",
        "            recon_images, _, _ = vae(sample_batch)\n",
        "        vutils.save_image(recon_images.detach().cpu(), os.path.join(OUTPUT_DIR, f\"vae_epoch_{epoch+1:03d}.png\"), normalize=True)\n",
        "    print(\"VAE Training Complete.\")\n",
        "\n",
        "def train_diffusion(dataloader, num_epochs=NUM_EPOCHS, noise_std=0.1):\n",
        "    print(\"Starting Diffusion Model Training (Advanced Approach)...\")\n",
        "    diffusion = DiffusionModel().to(device)\n",
        "    optimizerDiff = optim.Adam(diffusion.parameters(), lr=LR)\n",
        "    mse_loss = nn.MSELoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, (data, _) in enumerate(dataloader):\n",
        "            diffusion.zero_grad()\n",
        "            data = data.to(device)\n",
        "            # Simulate noisy input.\n",
        "            noise = torch.randn_like(data) * noise_std\n",
        "            noisy_data = data + noise\n",
        "            # For this simplified example, the model learns to recover the original image.\n",
        "            pred = diffusion(noisy_data)\n",
        "            loss = mse_loss(pred, data)\n",
        "            loss.backward()\n",
        "            optimizerDiff.step()\n",
        "            running_loss += loss.item()\n",
        "            if i % 100 == 0:\n",
        "                print(f\"[Diffusion][Epoch {epoch+1}/{num_epochs}][Batch {i}/{len(dataloader)}]: Loss: {loss.item():.4f}\")\n",
        "        epoch_loss = running_loss / len(dataloader)\n",
        "        print(f\"[Diffusion] Epoch [{epoch+1}/{num_epochs}] Average Loss: {epoch_loss:.4f}\")\n",
        "        # Evaluate: Save denoised images from the last batch.\n",
        "        with torch.no_grad():\n",
        "            denoised = diffusion(noisy_data).detach().cpu()\n",
        "        vutils.save_image(denoised, os.path.join(OUTPUT_DIR, f\"diffusion_epoch_{epoch+1:03d}.png\"), normalize=True)\n",
        "    print(\"Diffusion Model Training Complete.\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Load CIFAR-10 dataloader (Section 2).\n",
        "    dataloader = get_cifar10_dataloader()\n",
        "\n",
        "    # 4.1. Baseline Approach: Train the baseline GAN.\n",
        "    train_gan(dataloader)\n",
        "\n",
        "    train_vae(dataloader)\n",
        "\n",
        "    train_diffusion(dataloader)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save each model's state dictionary.\n",
        "torch.save(gan_generator.state_dict(), os.path.join(OUTPUT_DIR, 'gan_generator.pth'))\n",
        "torch.save(gan_discriminator.state_dict(), os.path.join(OUTPUT_DIR, 'gan_discriminator.pth'))\n",
        "torch.save(vae_model.state_dict(), os.path.join(OUTPUT_DIR, 'vae.pth'))\n",
        "torch.save(diffusion_model.state_dict(), os.path.join(OUTPUT_DIR, 'diffusion.pth'))\n",
        "print(\"Models saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "AQKNzeA4NqlC",
        "outputId": "17427c77-79a3-4434-9cc1-f1217902b915"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'gan_generator' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-3949a06a595a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save each model's state dictionary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgan_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gan_generator.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgan_discriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gan_discriminator.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvae_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vae.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiffusion_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'diffusion.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gan_generator' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define the output folder.\n",
        "OUTPUT_DIR = './output'\n",
        "\n",
        "# Define subdirectories for each model.\n",
        "gan_folder = os.path.join(OUTPUT_DIR, 'gan')\n",
        "vae_folder = os.path.join(OUTPUT_DIR, 'vae')\n",
        "diffusion_folder = os.path.join(OUTPUT_DIR, 'diffusion')\n",
        "\n",
        "# Create the subdirectories if they don't exist.\n",
        "os.makedirs(gan_folder, exist_ok=True)\n",
        "os.makedirs(vae_folder, exist_ok=True)\n",
        "os.makedirs(diffusion_folder, exist_ok=True)\n",
        "\n",
        "# Iterate over all items in the output directory.\n",
        "for file_name in os.listdir(OUTPUT_DIR):\n",
        "    full_path = os.path.join(OUTPUT_DIR, file_name)\n",
        "    # Skip directories\n",
        "    if not os.path.isfile(full_path):\n",
        "        continue\n",
        "\n",
        "    # Check filename prefix and move accordingly.\n",
        "    if file_name.startswith('gan_'):\n",
        "        destination = os.path.join(gan_folder, file_name)\n",
        "        shutil.move(full_path, destination)\n",
        "        print(f\"Moved {file_name} to {gan_folder}\")\n",
        "    elif file_name.startswith('vae_'):\n",
        "        destination = os.path.join(vae_folder, file_name)\n",
        "        shutil.move(full_path, destination)\n",
        "        print(f\"Moved {file_name} to {vae_folder}\")\n",
        "    elif file_name.startswith('diffusion_'):\n",
        "        destination = os.path.join(diffusion_folder, file_name)\n",
        "        shutil.move(full_path, destination)\n",
        "        print(f\"Moved {file_name} to {diffusion_folder}\")\n",
        "\n",
        "print(\"File reorganization complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xgi7j-_7XnHD",
        "outputId": "13a12258-b075-4b45-cb1f-1e4cd78603ab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moved vae_epoch_037.png to ./output/vae\n",
            "Moved diffusion_epoch_020.png to ./output/diffusion\n",
            "Moved diffusion_epoch_031.png to ./output/diffusion\n",
            "Moved gan_epoch_026.png to ./output/gan\n",
            "Moved diffusion_epoch_034.png to ./output/diffusion\n",
            "Moved vae_epoch_046.png to ./output/vae\n",
            "Moved diffusion_epoch_022.png to ./output/diffusion\n",
            "Moved diffusion_epoch_028.png to ./output/diffusion\n",
            "Moved diffusion_epoch_013.png to ./output/diffusion\n",
            "Moved gan_epoch_015.png to ./output/gan\n",
            "Moved diffusion_epoch_026.png to ./output/diffusion\n",
            "Moved vae_epoch_005.png to ./output/vae\n",
            "Moved diffusion_epoch_027.png to ./output/diffusion\n",
            "Moved vae_epoch_003.png to ./output/vae\n",
            "Moved vae_epoch_034.png to ./output/vae\n",
            "Moved vae_epoch_018.png to ./output/vae\n",
            "Moved diffusion_epoch_044.png to ./output/diffusion\n",
            "Moved diffusion_epoch_048.png to ./output/diffusion\n",
            "Moved diffusion_epoch_021.png to ./output/diffusion\n",
            "Moved vae_epoch_049.png to ./output/vae\n",
            "Moved gan_epoch_045.png to ./output/gan\n",
            "Moved diffusion_epoch_039.png to ./output/diffusion\n",
            "Moved gan_epoch_019.png to ./output/gan\n",
            "Moved vae_epoch_020.png to ./output/vae\n",
            "Moved vae_epoch_001.png to ./output/vae\n",
            "Moved gan_epoch_029.png to ./output/gan\n",
            "Moved diffusion_epoch_043.png to ./output/diffusion\n",
            "Moved vae_epoch_043.png to ./output/vae\n",
            "Moved gan_epoch_036.png to ./output/gan\n",
            "Moved diffusion_epoch_025.png to ./output/diffusion\n",
            "Moved gan_epoch_048.png to ./output/gan\n",
            "Moved diffusion_epoch_007.png to ./output/diffusion\n",
            "Moved gan_epoch_022.png to ./output/gan\n",
            "Moved diffusion_epoch_019.png to ./output/diffusion\n",
            "Moved diffusion_epoch_035.png to ./output/diffusion\n",
            "Moved vae_epoch_004.png to ./output/vae\n",
            "Moved vae_epoch_029.png to ./output/vae\n",
            "Moved vae_epoch_021.png to ./output/vae\n",
            "Moved gan_epoch_018.png to ./output/gan\n",
            "Moved gan_epoch_003.png to ./output/gan\n",
            "Moved vae_epoch_048.png to ./output/vae\n",
            "Moved gan_epoch_017.png to ./output/gan\n",
            "Moved gan_epoch_040.png to ./output/gan\n",
            "Moved gan_epoch_049.png to ./output/gan\n",
            "Moved diffusion_epoch_009.png to ./output/diffusion\n",
            "Moved gan_epoch_035.png to ./output/gan\n",
            "Moved gan_epoch_050.png to ./output/gan\n",
            "Moved gan_epoch_014.png to ./output/gan\n",
            "Moved diffusion_epoch_008.png to ./output/diffusion\n",
            "Moved diffusion_epoch_006.png to ./output/diffusion\n",
            "Moved gan_epoch_039.png to ./output/gan\n",
            "Moved vae_epoch_013.png to ./output/vae\n",
            "Moved vae_epoch_032.png to ./output/vae\n",
            "Moved vae_epoch_039.png to ./output/vae\n",
            "Moved vae_epoch_026.png to ./output/vae\n",
            "Moved vae_epoch_050.png to ./output/vae\n",
            "Moved diffusion_epoch_016.png to ./output/diffusion\n",
            "Moved vae_epoch_002.png to ./output/vae\n",
            "Moved gan_epoch_044.png to ./output/gan\n",
            "Moved gan_epoch_032.png to ./output/gan\n",
            "Moved diffusion_epoch_001.png to ./output/diffusion\n",
            "Moved gan_epoch_028.png to ./output/gan\n",
            "Moved diffusion_epoch_049.png to ./output/diffusion\n",
            "Moved vae_epoch_008.png to ./output/vae\n",
            "Moved gan_epoch_030.png to ./output/gan\n",
            "Moved diffusion_epoch_045.png to ./output/diffusion\n",
            "Moved vae_epoch_016.png to ./output/vae\n",
            "Moved gan_epoch_001.png to ./output/gan\n",
            "Moved vae_epoch_023.png to ./output/vae\n",
            "Moved diffusion_epoch_011.png to ./output/diffusion\n",
            "Moved gan_epoch_038.png to ./output/gan\n",
            "Moved diffusion_epoch_036.png to ./output/diffusion\n",
            "Moved vae_epoch_007.png to ./output/vae\n",
            "Moved diffusion_epoch_023.png to ./output/diffusion\n",
            "Moved gan_epoch_034.png to ./output/gan\n",
            "Moved diffusion_epoch_047.png to ./output/diffusion\n",
            "Moved gan_epoch_037.png to ./output/gan\n",
            "Moved gan_epoch_021.png to ./output/gan\n",
            "Moved vae_epoch_031.png to ./output/vae\n",
            "Moved diffusion_epoch_037.png to ./output/diffusion\n",
            "Moved gan_epoch_046.png to ./output/gan\n",
            "Moved gan_epoch_027.png to ./output/gan\n",
            "Moved gan_epoch_011.png to ./output/gan\n",
            "Moved vae_epoch_012.png to ./output/vae\n",
            "Moved diffusion_epoch_002.png to ./output/diffusion\n",
            "Moved gan_epoch_023.png to ./output/gan\n",
            "Moved gan_epoch_024.png to ./output/gan\n",
            "Moved vae_epoch_014.png to ./output/vae\n",
            "Moved diffusion_epoch_038.png to ./output/diffusion\n",
            "Moved diffusion_epoch_029.png to ./output/diffusion\n",
            "Moved vae_epoch_038.png to ./output/vae\n",
            "Moved diffusion_epoch_012.png to ./output/diffusion\n",
            "Moved vae_epoch_045.png to ./output/vae\n",
            "Moved diffusion_epoch_050.png to ./output/diffusion\n",
            "Moved vae_epoch_042.png to ./output/vae\n",
            "Moved vae_epoch_047.png to ./output/vae\n",
            "Moved gan_epoch_042.png to ./output/gan\n",
            "Moved vae_epoch_033.png to ./output/vae\n",
            "Moved diffusion_epoch_033.png to ./output/diffusion\n",
            "Moved vae_epoch_011.png to ./output/vae\n",
            "Moved vae_epoch_017.png to ./output/vae\n",
            "Moved vae_epoch_025.png to ./output/vae\n",
            "Moved diffusion_epoch_014.png to ./output/diffusion\n",
            "Moved vae_epoch_019.png to ./output/vae\n",
            "Moved gan_epoch_009.png to ./output/gan\n",
            "Moved diffusion_epoch_032.png to ./output/diffusion\n",
            "Moved gan_epoch_041.png to ./output/gan\n",
            "Moved vae_epoch_036.png to ./output/vae\n",
            "Moved gan_epoch_005.png to ./output/gan\n",
            "Moved vae_epoch_044.png to ./output/vae\n",
            "Moved diffusion_epoch_004.png to ./output/diffusion\n",
            "Moved vae_epoch_006.png to ./output/vae\n",
            "Moved vae_epoch_027.png to ./output/vae\n",
            "Moved vae_epoch_028.png to ./output/vae\n",
            "Moved diffusion_epoch_017.png to ./output/diffusion\n",
            "Moved diffusion_epoch_042.png to ./output/diffusion\n",
            "Moved gan_epoch_025.png to ./output/gan\n",
            "Moved vae_epoch_015.png to ./output/vae\n",
            "Moved diffusion_epoch_003.png to ./output/diffusion\n",
            "Moved gan_epoch_031.png to ./output/gan\n",
            "Moved diffusion_epoch_015.png to ./output/diffusion\n",
            "Moved gan_epoch_010.png to ./output/gan\n",
            "Moved gan_epoch_047.png to ./output/gan\n",
            "Moved gan_epoch_016.png to ./output/gan\n",
            "Moved vae_epoch_010.png to ./output/vae\n",
            "Moved diffusion_epoch_040.png to ./output/diffusion\n",
            "Moved gan_epoch_007.png to ./output/gan\n",
            "Moved diffusion_epoch_010.png to ./output/diffusion\n",
            "Moved gan_epoch_043.png to ./output/gan\n",
            "Moved gan_epoch_002.png to ./output/gan\n",
            "Moved gan_epoch_033.png to ./output/gan\n",
            "Moved vae_epoch_030.png to ./output/vae\n",
            "Moved gan_epoch_012.png to ./output/gan\n",
            "Moved gan_epoch_020.png to ./output/gan\n",
            "Moved diffusion_epoch_018.png to ./output/diffusion\n",
            "Moved gan_epoch_013.png to ./output/gan\n",
            "Moved diffusion_epoch_046.png to ./output/diffusion\n",
            "Moved vae_epoch_009.png to ./output/vae\n",
            "Moved vae_epoch_035.png to ./output/vae\n",
            "Moved gan_epoch_004.png to ./output/gan\n",
            "Moved diffusion_epoch_041.png to ./output/diffusion\n",
            "Moved gan_epoch_006.png to ./output/gan\n",
            "Moved vae_epoch_040.png to ./output/vae\n",
            "Moved diffusion_epoch_024.png to ./output/diffusion\n",
            "Moved diffusion_epoch_005.png to ./output/diffusion\n",
            "Moved vae_epoch_041.png to ./output/vae\n",
            "Moved vae_epoch_022.png to ./output/vae\n",
            "Moved diffusion_epoch_030.png to ./output/diffusion\n",
            "Moved vae_epoch_024.png to ./output/vae\n",
            "Moved gan_epoch_008.png to ./output/gan\n",
            "File reorganization complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "DATA_PATH = './data'             # Where CIFAR-10 tar/pickle files are located\n",
        "IMAGE_SIZE = 32                  # Native CIFAR-10 resolution; set to 64\n",
        "OUTPUT_REAL_IMAGES = './data/real_images'   # Folder to store extracted images\n",
        "\n",
        "os.makedirs(OUTPUT_REAL_IMAGES, exist_ok=True)\n",
        "\n",
        "def export_cifar10_as_images():\n",
        "    # Transform pipeline\n",
        "    transform_pipe = transforms.Compose([\n",
        "        transforms.Resize(IMAGE_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    # Load CIFAR-10 training or test set\n",
        "    cifar_dataset = datasets.CIFAR10(root=DATA_PATH, train=True, download=True, transform=transform_pipe)\n",
        "    dataloader = DataLoader(cifar_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    count = 0\n",
        "    for images, labels in dataloader:\n",
        "        # images: [1, 3, H, W]\n",
        "        # Save image to disk as a PNG (in [0,1] range by default when using transforms.ToTensor())\n",
        "        save_image(images[0], os.path.join(OUTPUT_REAL_IMAGES, f\"img_{count:05d}.png\"))\n",
        "        count += 1\n",
        "\n",
        "    print(f\"Exported {count} images to {OUTPUT_REAL_IMAGES}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    export_cifar10_as_images()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLqSgKqaLUgK",
        "outputId": "071157ee-76f6-4e4f-ce47-66fa037289d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exported 50000 images to ./data/real_images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%whos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajXAyr6onm_t",
        "outputId": "f41680c0-8ba1-498b-90b5-d0513d2030e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variable                    Type        Data/Info\n",
            "-------------------------------------------------\n",
            "BATCH_SIZE                  int         128\n",
            "BETA1                       float       0.5\n",
            "CHANNELS_IMG                int         3\n",
            "DATA_PATH                   str         ./data\n",
            "DataLoader                  type        <class 'torch.utils.data.dataloader.DataLoader'>\n",
            "DiffusionModel              type        <class '__main__.DiffusionModel'>\n",
            "GANDiscriminator            type        <class '__main__.GANDiscriminator'>\n",
            "GANGenerator                type        <class '__main__.GANGenerator'>\n",
            "IMAGE_SIZE                  int         32\n",
            "LR                          float       0.0002\n",
            "NOISE_DIM                   int         100\n",
            "NUM_EPOCHS                  int         50\n",
            "OUTPUT_DIR                  str         ./output\n",
            "OUTPUT_REAL_IMAGES          str         ./data/real_images\n",
            "SelfAttention               type        <class '__main__.SelfAttention'>\n",
            "UNetBlock                   type        <class '__main__.UNetBlock'>\n",
            "VAE                         type        <class '__main__.VAE'>\n",
            "datasets                    module      <module 'torchvision.data<...>on/datasets/__init__.py'>\n",
            "destination                 str         ./output/gan/gan_epoch_049.png\n",
            "device                      device      cuda\n",
            "diffusion_folder            str         ./output/diffusion\n",
            "evaluate_generated_images   function    <function evaluate_genera<...>images at 0x7f43b7e95120>\n",
            "export_cifar10_as_images    function    <function export_cifar10_<...>images at 0x7f43828eeca0>\n",
            "file_name                   str         gan_epoch_049.png\n",
            "full_path                   str         ./output/gan_epoch_049.png\n",
            "gan_folder                  str         ./output/gan\n",
            "get_cifar10_dataloader      function    <function get_cifar10_dat<...>loader at 0x7f43b7e94fe0>\n",
            "main                        function    <function main at 0x7f43884c0c20>\n",
            "nn                          module      <module 'torch.nn' from '<...>es/torch/nn/__init__.py'>\n",
            "optim                       module      <module 'torch.optim' fro<...>torch/optim/__init__.py'>\n",
            "os                          module      <module 'os' (frozen)>\n",
            "plt                         module      <module 'matplotlib.pyplo<...>es/matplotlib/pyplot.py'>\n",
            "random                      module      <module 'random' from '/u<...>ib/python3.11/random.py'>\n",
            "save_image                  function    <function save_image at 0x7f43c40f1080>\n",
            "seed                        int         42\n",
            "shutil                      module      <module 'shutil' from '/u<...>ib/python3.11/shutil.py'>\n",
            "torch                       module      <module 'torch' from '/us<...>kages/torch/__init__.py'>\n",
            "train_diffusion             function    <function train_diffusion at 0x7f43884c0b80>\n",
            "train_gan                   function    <function train_gan at 0x7f43884c09a0>\n",
            "train_vae                   function    <function train_vae at 0x7f43884c0a40>\n",
            "transforms                  module      <module 'torchvision.tran<...>/transforms/__init__.py'>\n",
            "vae_folder                  str         ./output/vae\n",
            "vutils                      module      <module 'torchvision.util<...>es/torchvision/utils.py'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "\"\"\"\n",
        "Evaluate Generated Images from Subdirectories using pytorch_image_generation_metrics\n",
        "\n",
        "This script computes the Inception Score (IS) and Frechet Inception Distance (FID)\n",
        "for generated images stored in subdirectories of the output directory. It assumes\n",
        "that your images are stored as follows:\n",
        "\n",
        "    ./output/gan/\n",
        "    ./output/vae/\n",
        "    ./output/diffusion/\n",
        "\n",
        "The script uses the functions:\n",
        "    - get_inception_score_from_directory\n",
        "    - get_fid_from_directory\n",
        "    - get_inception_score_and_fid_from_directory\n",
        "\n",
        "A reference NPZ file (with precomputed real-image statistics) is required for FID.\n",
        "For CIFAR-10, you might use something like './data/cifar10_real_stats.npz'.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from pytorch_image_generation_metrics import (\n",
        "    get_inception_score_from_directory,\n",
        "    get_fid_from_directory,\n",
        "    get_inception_score_and_fid_from_directory\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Directory and Reference File Configuration\n",
        "# ---------------------------------------------------------------------------\n",
        "OUTPUT_DIR = './output'\n",
        "GAN_DIR = os.path.join(OUTPUT_DIR, \"gan\")\n",
        "VAE_DIR = os.path.join(OUTPUT_DIR, \"vae\")\n",
        "DIFFUSION_DIR = os.path.join(OUTPUT_DIR, \"diffusion\")\n",
        "\n",
        "# Path to the NPZ file with real-image statistics.\n",
        "FID_REF = './data/cifar10_real_stats.npz'\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Evaluation Function\n",
        "# ---------------------------------------------------------------------------\n",
        "def evaluate_images_dir(model_name, img_dir, fid_ref):\n",
        "    \"\"\"\n",
        "    Evaluate generated images in a directory using pytorch_image_generation_metrics.\n",
        "\n",
        "    Parameters:\n",
        "        model_name (str): Identifier for the model type (e.g., \"GAN\", \"VAE\", \"Diffusion\").\n",
        "        img_dir (str): Directory path containing generated images.\n",
        "        fid_ref (str): Path to the NPZ file with precomputed statistics for real images.\n",
        "    \"\"\"\n",
        "    print(f\"\\nEvaluating {model_name} images from directory: {img_dir}\\n\")\n",
        "\n",
        "    # Compute the Inception Score from the directory.\n",
        "    try:\n",
        "        IS, IS_std = get_inception_score_from_directory(img_dir, batch_size=32, resize=True, splits=10)\n",
        "        print(f\"[{model_name}] Inception Score: {IS:.2f} ± {IS_std:.2f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error computing Inception Score for {model_name}: {e}\")\n",
        "\n",
        "    # Compute the FID from the directory.\n",
        "    try:\n",
        "        FID = get_fid_from_directory(img_dir, fid_ref, batch_size=32, resize=True)\n",
        "        print(f\"[{model_name}] FID: {FID:.2f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error computing FID for {model_name}: {e}\")\n",
        "\n",
        "    # Optionally, compute both metrics together.\n",
        "    try:\n",
        "        (IS2, IS_std2), FID2 = get_inception_score_and_fid_from_directory(img_dir, fid_ref, batch_size=32, resize=True, splits=10)\n",
        "        print(f\"[{model_name}] Combined -> IS: {IS2:.2f} ± {IS_std2:.2f}, FID: {FID2:.2f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error computing combined metrics for {model_name}: {e}\")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Main Execution\n",
        "# ---------------------------------------------------------------------------\n",
        "def main():\n",
        "    evaluate_images_dir(\"GAN\", GAN_DIR, FID_REF)\n",
        "    evaluate_images_dir(\"VAE\", VAE_DIR, FID_REF)\n",
        "    evaluate_images_dir(\"Diffusion\", DIFFUSION_DIR, FID_REF)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xF3kh5HBJPa7",
        "outputId": "7ecac58a-3296-4084-fc75-cde8616af268"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating GAN images from directory: ./output/gan\n",
            "\n",
            "Error computing Inception Score for GAN: get_inception_feature() got an unexpected keyword argument 'resize'\n",
            "Error computing FID for GAN: get_inception_feature() got an unexpected keyword argument 'resize'\n",
            "Error computing combined metrics for GAN: get_inception_feature() got an unexpected keyword argument 'resize'\n",
            "\n",
            "Evaluating VAE images from directory: ./output/vae\n",
            "\n",
            "Error computing Inception Score for VAE: get_inception_feature() got an unexpected keyword argument 'resize'\n",
            "Error computing FID for VAE: get_inception_feature() got an unexpected keyword argument 'resize'\n",
            "Error computing combined metrics for VAE: get_inception_feature() got an unexpected keyword argument 'resize'\n",
            "\n",
            "Evaluating Diffusion images from directory: ./output/diffusion\n",
            "\n",
            "Error computing Inception Score for Diffusion: get_inception_feature() got an unexpected keyword argument 'resize'\n",
            "Error computing FID for Diffusion: get_inception_feature() got an unexpected keyword argument 'resize'\n",
            "Error computing combined metrics for Diffusion: get_inception_feature() got an unexpected keyword argument 'resize'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install\n",
        "from pytorch_image_generation_metrics import (\n",
        "    get_inception_score_from_directory,\n",
        "    get_fid_from_directory\n",
        ")\n",
        "\n",
        "# Define directories for each model's output\n",
        "GAN_DIR = './output/gan'\n",
        "VAE_DIR = './output/vae'\n",
        "DIFFUSION_DIR = './output/diffusion'\n",
        "\n",
        "# Path to the NPZ file with precomputed real-image statistics (e.g., CIFAR-10 test set)\n",
        "FID_REF = '/content/cifar10.test.npz'\n",
        "\n",
        "# --------------------\n",
        "# Evaluate GAN Outputs\n",
        "# --------------------\n",
        "IS_gan, IS_std_gan = get_inception_score_from_directory(GAN_DIR, batch_size=32, splits=10)\n",
        "print(f\"GAN Inception Score: {IS_gan:.2f} ± {IS_std_gan:.2f}\")\n",
        "\n",
        "FID_gan = get_fid_from_directory(GAN_DIR, FID_REF, batch_size=32)\n",
        "print(f\"GAN FID: {FID_gan:.2f}\")\n",
        "\n",
        "# --------------------\n",
        "# Evaluate VAE Outputs\n",
        "# --------------------\n",
        "IS_vae, IS_std_vae = get_inception_score_from_directory(VAE_DIR, batch_size=32, splits=10)\n",
        "print(f\"VAE Inception Score: {IS_vae:.2f} ± {IS_std_vae:.2f}\")\n",
        "\n",
        "FID_vae = get_fid_from_directory(VAE_DIR, FID_REF, batch_size=32)\n",
        "print(f\"VAE FID: {FID_vae:.2f}\")\n",
        "\n",
        "# --------------------\n",
        "# Evaluate Diffusion Outputs\n",
        "# --------------------\n",
        "IS_diff, IS_std_diff = get_inception_score_from_directory(DIFFUSION_DIR, batch_size=32, splits=10)\n",
        "print(f\"Diffusion Inception Score: {IS_diff:.2f} ± {IS_std_diff:.2f}\")\n",
        "\n",
        "FID_diff = get_fid_from_directory(DIFFUSION_DIR, FID_REF, batch_size=32)\n",
        "print(f\"Diffusion FID: {FID_diff:.2f}\")"
      ],
      "metadata": {
        "id": "c0t8ugtUnMPK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf9ca80c-4038-4db6-ca15-7e4d5e8de437"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: You must give at least one requirement to install (see \"pip help install\")\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/w86763777/pytorch-image-generation-metrics/releases/download/v0.1.0/pt_inception-2015-12-05-6726825d.pth\" to /root/.cache/torch/hub/checkpoints/pt_inception-2015-12-05-6726825d.pth\n",
            "100%|██████████| 91.2M/91.2M [00:02<00:00, 39.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GAN Inception Score: 1.49 ± 0.16\n",
            "GAN FID: 475.42\n",
            "VAE Inception Score: 1.20 ± 0.08\n",
            "VAE FID: 435.53\n",
            "Diffusion Inception Score: 1.22 ± 0.05\n",
            "Diffusion FID: 466.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "\"\"\"\n",
        "Project Title: Benchmarking Generative Models on CelebA\n",
        "\n",
        "This script unzips a local CelebA archive, trains a DCGAN, VAE, and\n",
        "a simple Diffusion Model (with attention) on the CelebA images (resized\n",
        "to 64×64, normalized to [-1,1]), and saves composite outputs after each epoch.\n",
        "\n",
        "Requirements:\n",
        "    pip install torch torchvision matplotlib pytorch-gan-metrics\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import random\n",
        "import zipfile\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, utils as vutils\n",
        "\n",
        "# ---------------------\n",
        "# 1. Configuration\n",
        "# ---------------------\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "DATA_PATH    = './data'\n",
        "CELEBA_ZIP   = os.path.join(DATA_PATH, 'img_align_celeba.zip')\n",
        "CELEBA_DIR   = os.path.join(DATA_PATH, 'celeba')\n",
        "OUTPUT_DIR   = './output1'   # Updated output directory\n",
        "os.makedirs(CELEBA_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "BATCH_SIZE   = 128\n",
        "IMAGE_SIZE   = 64\n",
        "NOISE_DIM    = 100\n",
        "NUM_EPOCHS   = 30\n",
        "LR           = 0.0002\n",
        "BETA1        = 0.5\n",
        "CHANNELS_IMG = 3\n",
        "device       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ---------------------\n",
        "# 2. Unzip CelebA\n",
        "# ---------------------\n",
        "if not os.listdir(CELEBA_DIR):\n",
        "    print(f\"Extracting {CELEBA_ZIP} → {CELEBA_DIR}\")\n",
        "    with zipfile.ZipFile(CELEBA_ZIP, 'r') as z:\n",
        "        z.extractall(CELEBA_DIR)\n",
        "\n",
        "# ---------------------\n",
        "# 3. DataLoader\n",
        "# ---------------------\n",
        "def get_celeba_dataloader(root_dir=CELEBA_DIR, batch_size=BATCH_SIZE, image_size=IMAGE_SIZE):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.CenterCrop(178),\n",
        "        transforms.Resize(image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "    ])\n",
        "    dataset = datasets.ImageFolder(root_dir, transform=transform)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "# ---------------------\n",
        "# 4. Model Definitions\n",
        "# (Insert your GANGenerator, GANDiscriminator, VAE, UNetBlock, SelfAttention, DiffusionModel classes here)\n",
        "# ---------------------\n",
        "\n",
        "# ---------------------\n",
        "# 5. Evaluation Utility\n",
        "# ---------------------\n",
        "def evaluate_generated_images(generator, fixed_noise, output_dir=OUTPUT_DIR, epoch=0):\n",
        "    generator.eval()\n",
        "    with torch.no_grad():\n",
        "        imgs = generator(fixed_noise).cpu()\n",
        "    path = os.path.join(output_dir, f\"celeba_gan_epoch_{epoch:03d}.png\")\n",
        "    vutils.save_image(imgs, path, normalize=True, nrow=8)\n",
        "    print(f\"Saved {path}\")\n",
        "\n",
        "# ---------------------\n",
        "# 6. Training Loops\n",
        "# ---------------------\n",
        "def train_gan(dataloader):\n",
        "    netG = GANGenerator().to(device)\n",
        "    netD = GANDiscriminator().to(device)\n",
        "    optG = optim.Adam(netG.parameters(), lr=LR, betas=(BETA1,0.999))\n",
        "    optD = optim.Adam(netD.parameters(), lr=LR, betas=(BETA1,0.999))\n",
        "    criterion = nn.BCELoss()\n",
        "    fixed_noise = torch.randn(64, NOISE_DIM, 1, 1, device=device)\n",
        "\n",
        "    for epoch in range(1, NUM_EPOCHS+1):\n",
        "        for real, _ in dataloader:\n",
        "            b = real.size(0)\n",
        "            real = real.to(device)\n",
        "            # Discriminator real\n",
        "            netD.zero_grad()\n",
        "            label_real = torch.full((b,),1.,device=device)\n",
        "            lossD_real = criterion(netD(real).view(-1), label_real)\n",
        "            # Discriminator fake\n",
        "            noise = torch.randn(b, NOISE_DIM,1,1,device=device)\n",
        "            fake = netG(noise)\n",
        "            label_fake = torch.full((b,),0.,device=device)\n",
        "            lossD_fake = criterion(netD(fake.detach()).view(-1), label_fake)\n",
        "            (lossD_real+lossD_fake).backward()\n",
        "            optD.step()\n",
        "            # Generator update\n",
        "            netG.zero_grad()\n",
        "            label_gen = torch.full((b,),1.,device=device)\n",
        "            lossG = criterion(netD(fake).view(-1), label_gen)\n",
        "            lossG.backward(); optG.step()\n",
        "\n",
        "        print(f\"GAN Epoch {epoch}: Loss_D {lossD_real+lossD_fake:.4f}, Loss_G {lossG:.4f}\")\n",
        "        evaluate_generated_images(netG, fixed_noise, epoch=epoch)\n",
        "\n",
        "def train_vae(dataloader):\n",
        "    vae = VAE().to(device)\n",
        "    opt = optim.Adam(vae.parameters(), lr=LR)\n",
        "    for epoch in range(1, NUM_EPOCHS+1):\n",
        "        tot=0\n",
        "        for x,_ in dataloader:\n",
        "            x = x.to(device)\n",
        "            recon, mu, logvar = vae(x)\n",
        "            recon_loss = nn.MSELoss(reduction='sum')(recon,x)\n",
        "            kl = -0.5*torch.sum(1+logvar-mu.pow(2)-logvar.exp())\n",
        "            loss = recon_loss+kl\n",
        "            opt.zero_grad(); loss.backward(); opt.step()\n",
        "            tot+=loss.item()\n",
        "        print(f\"VAE Epoch {epoch}: AvgLoss {tot/len(dataloader.dataset):.4f}\")\n",
        "        with torch.no_grad():\n",
        "            sample = next(iter(dataloader))[0][:64].to(device)\n",
        "            imgs,_,_ = vae(sample)\n",
        "            vutils.save_image(imgs.cpu(), os.path.join(OUTPUT_DIR,f\"celeba_vae_{epoch:03d}.png\"), normalize=True)\n",
        "\n",
        "def train_diffusion(dataloader):\n",
        "    diff = DiffusionModel().to(device)\n",
        "    opt = optim.Adam(diff.parameters(), lr=LR)\n",
        "    for epoch in range(1, NUM_EPOCHS+1):\n",
        "        tot=0\n",
        "        for x,_ in dataloader:\n",
        "            x = x.to(device)\n",
        "            noisy = x + 0.1*torch.randn_like(x)\n",
        "            pred = diff(noisy)\n",
        "            loss = nn.MSELoss()(pred,x)\n",
        "            opt.zero_grad(); loss.backward(); opt.step()\n",
        "            tot+=loss.item()\n",
        "        print(f\"Diff Epoch {epoch}: AvgLoss {tot/len(dataloader):.4f}\")\n",
        "        with torch.no_grad():\n",
        "            noisy = next(iter(dataloader))[0][:64].to(device) + 0.1*torch.randn(64,CHANNELS_IMG,IMAGE_SIZE,IMAGE_SIZE,device=device)\n",
        "            out = diff(noisy)\n",
        "            vutils.save_image(out.cpu(), os.path.join(OUTPUT_DIR,f\"celeba_diff_{epoch:03d}.png\"), normalize=True)\n",
        "\n",
        "# ---------------------\n",
        "# 7. Main\n",
        "# ---------------------\n",
        "def main():\n",
        "    celeba_loader = get_celeba_dataloader()\n",
        "    train_gan(celeba_loader)\n",
        "    train_vae(celeba_loader)\n",
        "    train_diffusion(celeba_loader)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBXK8DwJpX5e",
        "outputId": "8a3d75bd-e5bb-4b22-f494-227f5bda0bb1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/img_align_celeba.zip → ./data/celeba\n",
            "GAN Epoch 1: Loss_D 0.3812, Loss_G 3.6866\n",
            "Saved ./output1/celeba_gan_epoch_001.png\n",
            "GAN Epoch 2: Loss_D 1.0079, Loss_G 1.8592\n",
            "Saved ./output1/celeba_gan_epoch_002.png\n",
            "GAN Epoch 3: Loss_D 0.6834, Loss_G 1.1695\n",
            "Saved ./output1/celeba_gan_epoch_003.png\n",
            "GAN Epoch 4: Loss_D 0.4188, Loss_G 3.2421\n",
            "Saved ./output1/celeba_gan_epoch_004.png\n",
            "GAN Epoch 5: Loss_D 0.5022, Loss_G 2.5650\n",
            "Saved ./output1/celeba_gan_epoch_005.png\n",
            "GAN Epoch 6: Loss_D 0.4067, Loss_G 5.3215\n",
            "Saved ./output1/celeba_gan_epoch_006.png\n",
            "GAN Epoch 7: Loss_D 0.4896, Loss_G 4.8496\n",
            "Saved ./output1/celeba_gan_epoch_007.png\n",
            "GAN Epoch 8: Loss_D 1.6375, Loss_G 4.0988\n",
            "Saved ./output1/celeba_gan_epoch_008.png\n",
            "GAN Epoch 9: Loss_D 1.2593, Loss_G 4.8197\n",
            "Saved ./output1/celeba_gan_epoch_009.png\n",
            "GAN Epoch 10: Loss_D 0.2019, Loss_G 3.6329\n",
            "Saved ./output1/celeba_gan_epoch_010.png\n",
            "GAN Epoch 11: Loss_D 0.2061, Loss_G 3.9808\n",
            "Saved ./output1/celeba_gan_epoch_011.png\n",
            "GAN Epoch 12: Loss_D 1.0059, Loss_G 5.4409\n",
            "Saved ./output1/celeba_gan_epoch_012.png\n",
            "GAN Epoch 13: Loss_D 0.3385, Loss_G 3.8920\n",
            "Saved ./output1/celeba_gan_epoch_013.png\n",
            "GAN Epoch 14: Loss_D 0.1200, Loss_G 3.6154\n",
            "Saved ./output1/celeba_gan_epoch_014.png\n",
            "GAN Epoch 15: Loss_D 0.1746, Loss_G 3.4265\n",
            "Saved ./output1/celeba_gan_epoch_015.png\n",
            "GAN Epoch 16: Loss_D 0.4114, Loss_G 3.0787\n",
            "Saved ./output1/celeba_gan_epoch_016.png\n",
            "GAN Epoch 17: Loss_D 0.1389, Loss_G 5.6183\n",
            "Saved ./output1/celeba_gan_epoch_017.png\n",
            "GAN Epoch 18: Loss_D 0.0552, Loss_G 4.7947\n",
            "Saved ./output1/celeba_gan_epoch_018.png\n",
            "GAN Epoch 19: Loss_D 0.1282, Loss_G 6.3955\n",
            "Saved ./output1/celeba_gan_epoch_019.png\n",
            "GAN Epoch 20: Loss_D 0.0622, Loss_G 5.8693\n",
            "Saved ./output1/celeba_gan_epoch_020.png\n",
            "GAN Epoch 21: Loss_D 0.1654, Loss_G 3.8760\n",
            "Saved ./output1/celeba_gan_epoch_021.png\n",
            "GAN Epoch 22: Loss_D 0.0731, Loss_G 4.5956\n",
            "Saved ./output1/celeba_gan_epoch_022.png\n",
            "GAN Epoch 23: Loss_D 0.0687, Loss_G 5.1600\n",
            "Saved ./output1/celeba_gan_epoch_023.png\n",
            "GAN Epoch 24: Loss_D 0.3061, Loss_G 2.7640\n",
            "Saved ./output1/celeba_gan_epoch_024.png\n",
            "GAN Epoch 25: Loss_D 0.0715, Loss_G 5.5359\n",
            "Saved ./output1/celeba_gan_epoch_025.png\n",
            "GAN Epoch 26: Loss_D 0.0440, Loss_G 5.6663\n",
            "Saved ./output1/celeba_gan_epoch_026.png\n",
            "GAN Epoch 27: Loss_D 0.3188, Loss_G 3.7211\n",
            "Saved ./output1/celeba_gan_epoch_027.png\n",
            "GAN Epoch 28: Loss_D 0.0399, Loss_G 5.7610\n",
            "Saved ./output1/celeba_gan_epoch_028.png\n",
            "GAN Epoch 29: Loss_D 0.0981, Loss_G 3.8667\n",
            "Saved ./output1/celeba_gan_epoch_029.png\n",
            "GAN Epoch 30: Loss_D 0.1246, Loss_G 4.6814\n",
            "Saved ./output1/celeba_gan_epoch_030.png\n",
            "VAE Epoch 1: AvgLoss 781.7878\n",
            "VAE Epoch 2: AvgLoss 523.0021\n",
            "VAE Epoch 3: AvgLoss 485.5755\n",
            "VAE Epoch 4: AvgLoss 468.8427\n",
            "VAE Epoch 5: AvgLoss 458.4064\n",
            "VAE Epoch 6: AvgLoss 451.1796\n",
            "VAE Epoch 7: AvgLoss 445.5022\n",
            "VAE Epoch 8: AvgLoss 441.2300\n",
            "VAE Epoch 9: AvgLoss 437.2574\n",
            "VAE Epoch 10: AvgLoss 434.3315\n",
            "VAE Epoch 11: AvgLoss 431.7902\n",
            "VAE Epoch 12: AvgLoss 429.4650\n",
            "VAE Epoch 13: AvgLoss 427.4264\n",
            "VAE Epoch 14: AvgLoss 425.7470\n",
            "VAE Epoch 15: AvgLoss 423.7398\n",
            "VAE Epoch 16: AvgLoss 422.5507\n",
            "VAE Epoch 17: AvgLoss 421.4197\n",
            "VAE Epoch 18: AvgLoss 420.0292\n",
            "VAE Epoch 19: AvgLoss 418.9600\n",
            "VAE Epoch 20: AvgLoss 417.9122\n",
            "VAE Epoch 21: AvgLoss 416.8851\n",
            "VAE Epoch 22: AvgLoss 415.9014\n",
            "VAE Epoch 23: AvgLoss 415.2553\n",
            "VAE Epoch 24: AvgLoss 414.3458\n",
            "VAE Epoch 25: AvgLoss 413.5497\n",
            "VAE Epoch 26: AvgLoss 412.9494\n",
            "VAE Epoch 27: AvgLoss 412.3030\n",
            "VAE Epoch 28: AvgLoss 411.5355\n",
            "VAE Epoch 29: AvgLoss 410.8462\n",
            "VAE Epoch 30: AvgLoss 410.3987\n",
            "Diff Epoch 1: AvgLoss 0.0058\n",
            "Diff Epoch 2: AvgLoss 0.0028\n",
            "Diff Epoch 3: AvgLoss 0.0025\n",
            "Diff Epoch 4: AvgLoss 0.0023\n",
            "Diff Epoch 5: AvgLoss 0.0021\n",
            "Diff Epoch 6: AvgLoss 0.0019\n",
            "Diff Epoch 7: AvgLoss 0.0017\n",
            "Diff Epoch 8: AvgLoss 0.0016\n",
            "Diff Epoch 9: AvgLoss 0.0016\n",
            "Diff Epoch 10: AvgLoss 0.0016\n",
            "Diff Epoch 11: AvgLoss 0.0015\n",
            "Diff Epoch 12: AvgLoss 0.0015\n",
            "Diff Epoch 13: AvgLoss 0.0015\n",
            "Diff Epoch 14: AvgLoss 0.0015\n",
            "Diff Epoch 15: AvgLoss 0.0014\n",
            "Diff Epoch 16: AvgLoss 0.0014\n",
            "Diff Epoch 17: AvgLoss 0.0014\n",
            "Diff Epoch 18: AvgLoss 0.0014\n",
            "Diff Epoch 19: AvgLoss 0.0014\n",
            "Diff Epoch 20: AvgLoss 0.0014\n",
            "Diff Epoch 21: AvgLoss 0.0014\n",
            "Diff Epoch 22: AvgLoss 0.0013\n",
            "Diff Epoch 23: AvgLoss 0.0013\n",
            "Diff Epoch 24: AvgLoss 0.0013\n",
            "Diff Epoch 25: AvgLoss 0.0013\n",
            "Diff Epoch 26: AvgLoss 0.0013\n",
            "Diff Epoch 27: AvgLoss 0.0013\n",
            "Diff Epoch 28: AvgLoss 0.0013\n",
            "Diff Epoch 29: AvgLoss 0.0013\n",
            "Diff Epoch 30: AvgLoss 0.0013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define the output folder.\n",
        "OUTPUT_DIR = './output1'\n",
        "\n",
        "# Define subdirectories for each model.\n",
        "gan_folder = os.path.join(OUTPUT_DIR, 'gan')\n",
        "vae_folder = os.path.join(OUTPUT_DIR, 'vae')\n",
        "diffusion_folder = os.path.join(OUTPUT_DIR, 'diffusion')\n",
        "\n",
        "# Create the subdirectories if they don't exist.\n",
        "os.makedirs(gan_folder, exist_ok=True)\n",
        "os.makedirs(vae_folder, exist_ok=True)\n",
        "os.makedirs(diffusion_folder, exist_ok=True)\n",
        "\n",
        "# Iterate over all items in the output directory.\n",
        "for file_name in os.listdir(OUTPUT_DIR):\n",
        "    full_path = os.path.join(OUTPUT_DIR, file_name)\n",
        "    # Skip directories\n",
        "    if not os.path.isfile(full_path):\n",
        "        continue\n",
        "\n",
        "    # Check filename prefix and move accordingly.\n",
        "    if file_name.startswith('gan_'):\n",
        "        destination = os.path.join(gan_folder, file_name)\n",
        "        shutil.move(full_path, destination)\n",
        "        print(f\"Moved {file_name} to {gan_folder}\")\n",
        "    elif file_name.startswith('vae_'):\n",
        "        destination = os.path.join(vae_folder, file_name)\n",
        "        shutil.move(full_path, destination)\n",
        "        print(f\"Moved {file_name} to {vae_folder}\")\n",
        "    elif file_name.startswith('diffusion_'):\n",
        "        destination = os.path.join(diffusion_folder, file_name)\n",
        "        shutil.move(full_path, destination)\n",
        "        print(f\"Moved {file_name} to {diffusion_folder}\")\n",
        "\n",
        "print(\"File reorganization complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BS1f6XcYroxs",
        "outputId": "91a03260-9c0f-4339-85da-8c3b5cf7e989"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File reorganization complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "BASE_DIR       = './output1'\n",
        "GAN_DIR        = os.path.join(BASE_DIR, 'gan')\n",
        "VAE_DIR        = os.path.join(BASE_DIR, 'vae')\n",
        "DIFFUSION_DIR  = os.path.join(BASE_DIR, 'diffusion')\n",
        "\n",
        "# 1. ensure folders exist\n",
        "for d in (GAN_DIR, VAE_DIR, DIFFUSION_DIR):\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "# 2. iterate and move\n",
        "for fname in os.listdir(BASE_DIR):\n",
        "    if not fname.lower().endswith('.png'):\n",
        "        continue\n",
        "\n",
        "    src = os.path.join(BASE_DIR, fname)\n",
        "    key = fname.lower()\n",
        "    if 'gan' in key:\n",
        "        dst_dir = GAN_DIR\n",
        "    elif 'vae' in key:\n",
        "        dst_dir = VAE_DIR\n",
        "    elif 'diff' in key:       # catches celeba_diff_*.png\n",
        "        dst_dir = DIFFUSION_DIR\n",
        "    else:\n",
        "        continue  # or handle “others” if needed\n",
        "\n",
        "    shutil.move(src, os.path.join(dst_dir, fname))\n",
        "    print(f\"Moved {fname} → {dst_dir}\")\n"
      ],
      "metadata": {
        "id": "lcWGtRh5ERKN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_image_generation_metrics import (\n",
        "    get_inception_score_from_directory,\n",
        "    get_fid_from_directory\n",
        ")\n",
        "\n",
        "# Define directories for each model's output\n",
        "GAN_DIR = './output1/gan'\n",
        "VAE_DIR = './output1/vae'\n",
        "DIFFUSION_DIR = './output1/diffusion'\n",
        "\n",
        "# Path to the NPZ file with precomputed real-image statistics (e.g., CIFAR-10 test set)\n",
        "FID_REF = '/content/celebahq.3k.128.npz'\n",
        "\n",
        "# --------------------\n",
        "# Evaluate GAN Outputs\n",
        "# --------------------\n",
        "IS_gan, IS_std_gan = get_inception_score_from_directory(GAN_DIR, batch_size=32, splits=10)\n",
        "print(f\"GAN Inception Score: {IS_gan:.2f} ± {IS_std_gan:.2f}\")\n",
        "\n",
        "FID_gan = get_fid_from_directory(GAN_DIR, FID_REF, batch_size=32)\n",
        "print(f\"GAN FID: {FID_gan:.2f}\")\n",
        "\n",
        "# --------------------\n",
        "# Evaluate VAE Outputs\n",
        "# --------------------\n",
        "IS_vae, IS_std_vae = get_inception_score_from_directory(VAE_DIR, batch_size=32, splits=10)\n",
        "print(f\"VAE Inception Score: {IS_vae:.2f} ± {IS_std_vae:.2f}\")\n",
        "\n",
        "FID_vae = get_fid_from_directory(VAE_DIR, FID_REF, batch_size=32)\n",
        "print(f\"VAE FID: {FID_vae:.2f}\")\n",
        "\n",
        "# --------------------\n",
        "# Evaluate Diffusion Oacutputs\n",
        "# --------------------\n",
        "IS_diff, IS_std_diff = get_inception_score_from_directory(DIFFUSION_DIR, batch_size=32, splits=10)\n",
        "print(f\"Diffusion Inception Score: {IS_diff:.2f} ± {IS_std_diff:.2f}\")\n",
        "\n",
        "FID_diff = get_fid_from_directory(DIFFUSION_DIR, FID_REF, batch_size=32)\n",
        "print(f\"Diffusion FID: {FID_diff:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLlUw6gFsZT-",
        "outputId": "2428c4ad-39ed-44bb-bf80-c05639d2e9fe"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GAN Inception Score: 1.20 ± 0.07\n",
            "GAN FID: 529.68\n",
            "VAE Inception Score: 1.12 ± 0.03\n",
            "VAE FID: 451.77\n",
            "Diffusion Inception Score: 1.29 ± 0.18\n",
            "Diffusion FID: 483.38\n"
          ]
        }
      ]
    }
  ]
}